---
title: "Reproducibility in plant pathology: where do we stand and a way forward"
author:
  - Adam H. Sparks:
      email: Adam.Sparks@dpird.wa.gov.au
      institute: DPIRD, USQ
      correspondence: true
  - Emerson Del Ponte:
      institute: UFV
  - Kaique dos Santos Alves:
      institute: UFV
  - Zachary S. L. Foster:
      institute: OSU
  - Niklaus J. Grünwald:
      institute: ARS
institute:
  - DPIRD: Department of Primary Industries and Regional Development, Farming Systems Innovation, Perth WA 6000, Australia
  - USQ: University of Southern Queensland, Centre for Crop Health, Toowoomba Qld 4350, Australia
  - UFV: Departmento de Fitopatologia, Universidade Federal de Viçosa, Brazil
  - OSU: Department of Botany and Plant Pathology, Oregon State University, Corvallis OR 97331, USA
  - ARS: Horticultural Crops Research Unit, USDA Agricultural Research Service, Corvallis OR 97330, USA
output: 
  bookdown::word_document2:
     fig_caption: yes
     number_sections: FALSE
     reference_docx: "../templates/template.docx" # Insert path for the DOCX file
     pandoc_args:
       - '--lua-filter=scholarly-metadata.lua'
       - '--lua-filter=author-info-blocks.lua'
       - '--lua-filter=pagebreak.lua'
bibliography: references.bib
link-citations: no
csl: "../templates/american-phytopathological-society.csl" # Insert path for the bib-style
---

```{r, setup, echo = FALSE, message=FALSE}
library("officer")
library("dplyr")
library("janitor")
library("flextable")
library("Reproducibility.in.Plant.Pathology")
library("tidyr")
library("DiagrammeR")
library("DiagrammeRsvg")
library("rsvg")
library("ggplot2")
library("patchwork")

knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  echo = FALSE,
  dev = c("postscript"),
  fig.width = 3.31,
  fig.height = 3.31,
  out.width = "100%",
  dpi = 300,
  comment = "#>",
  dev = "cairo_pdf",
  cache = TRUE
)
```

```{r fonts, include=FALSE, message=FALSE, eval=FALSE}
# Note 1: This chunk only needs to be run once on a computer to import fonts for
# use in R. This chunk makes .ttf fonts available to R for use and embedding in
# .eps figure outputs.
#
# Note 2: GhostScript needs to be installed at the system level for the PS files
# to be generated.  MacOS users can use `brew install ghostscript`.  Windows
# users should download and install. Linux users can use their package manager.
#
# Note 3: You may need to tell R where to find GhostScript's 'gs' executable.
# Set `R_GSCMD="/opt/homebrew/bin/gs"` in .Renviron for MacOS users using
# homebrew to install on a Mac M1.
# 
# Windows users can follow these directions:
# 1.	Go to the GhostScript website
#     <https://www.ghostscript.com/download/gsdnld.html>.
# 2.	Download the windows installer suitable for your machine
# 3.	Run the installer file which you downloaded and follow the prompts
# 4.	After running the installer click the windows "Start" button and type
#     "Edit environment variables for your account" and open
# 5.	In the tab 'Advanced' click the button at the bottom
#     'Environment Variables...'
# 6.	Under 'System variables' find the variable 'Path', select 'Path' and click
#     the 'Edit' button
# 7. 	Select a new line and copy the Ghostscript 'bin' folder location into the
#     field.
## 7.1	If you installed Ghostscript to the default folder location; then the
#     folder location will likely be "C:\Program Files\gs\gs9.52\bin", the
#     version number (9.52) may differ.
# 8.	Save and exit the environmental variables window
#
# This chunk is then run only if knitting on new computer that the files have
# not been generated

library("extrafont")
font_import()
```

**Keywords: open science, open data, reproducible research, FAIR data**

\newpage

# Abstract {-#abstract}

Open research practices have been highlighted extensively during the last ten years in many fields of scientific study as essential standards needed to promote transparency and reproducibility of scientific results. 
Scientific claims can only be evaluated based on how protocols, materials, equipment and methods were described; data were collected and prepared; and, analyses were conducted. 
Openly sharing protocols, data and computational code is central for current scholarly dissemination and communication, but in many fields, including plant pathology, adoption of these practices has been slow. 
We randomly selected 300 articles published from 2012 to 2018 across 21 journals representative of the plant pathology discipline and assigned them scores reflecting their openness and reproducibility. 
We found that most of the articles were not following protocols for open science, and were failing to share data or code in a reproducible way. 
We also propose that use of open-source tools facilitates reproducible work and analyses benefitting not just readers, but the authors as well.  
Finally, we also provide ideas and tools to promote open, reproducible research practices among plant pathologists.

\newpage

# Introduction

Modern plant pathological research has many facets given the array of disciplines and sub-disciplines currently involved.
Collectively, they contribute to increasing our basic and applied knowledge of several aspects of pathogen biology and disease development to ultimately improve plant disease management.
Scientific research in the field varies from the purely observational or descriptive nature to inferential, based on experimental or simulation-derived data sets.
Whatever the case, research findings are verifiable based on how much of the research materials, processes and outcomes are made available beyond what is reported in the scientific article and the ability of others to make use of the methods and results.
These research findings include biological materials (host and pathogen genotypes), nucleic/protein sequences, experimental and simulated raw data annotations, drawings and photographs and statistical analysis code among other materials and data generated during the course of the research.

Recently, open science has been highlighted with many donors expecting data to be available [@government_of_canada_2016; @vannoorden2017; @ARC2018], journals in the field promoting the sharing of data [@DelPonte2020], and other scientists interested in sharing and collaborating more widely [@Wald2010].

Reproducibility is one component under the umbrella of open science.
By proactively practising open science, the work increases the chance to become more reproducible through the availability of data and code.
That is, open science leads to reproducibility and replicability.

For us to easily discuss the topic, we first must define what we mean so that we may clearly communicate.
Many of the terms used in this area have varying definitions that may or may not agree with each other.
For instance, reproducible research was recently highlighted by many authors [@preeyanon2018; @Wallach2018; @Baker2016a; @Iqbal2016; @Nature2016; @Patil2016; @Weissgerber2016; @Brunsdon2015; @Sweedler2015; @Fitzjohn2014; @Ioannidis2014; @Fidler2013; @Stodden2013] as an important issue.

<!--# These references above are all prior 2017. We may want to update the list? I've added two using insert citation tool of the visual editor rmarkdown -->

In the biological sciences it is not always possible to use identical test material or perhaps the the time or resources are not available for full reproducibility, _e.g._, field trials that span years and locations or complex glasshouse experiments.
Therefore, we will follow Peng's [-@Peng2009] definition that provides clear guidelines for a minimum standard of "reproducible research":

> "The replication of scientific findings using independent investigators, methods, data, equipment, and protocols has long been, and will continue to be, the standard by which scientific claims are evaluated. However, in many fields of study there are examples of scientific investigations that cannot be fully replicated because of a lack of time or resources. In such a situation, there is a need for a minimum standard that can fill the void between full replication and nothing. One candidate for this minimum standard is 'reproducible research', which requires that data sets and computer code be made available to others for verifying published results and conducting alternative analyses".
> Peng, R.
> [-@Peng2009].
> *Reproducible research and Biostatistics*.
> Biostatistics, 10 (3): 405-408.

Therefore our definition of reproducibility will be that the computer code and data are made freely available to others for verification and conducting alternate analyses or for use in instructional purposes.
And that the software used are also easily obtained and preferably open source to avoid licensing or other issues related to accessibility for end-users related to costs or non-standard file formats, etc.

It goes without saying that plant pathologists already provide information on protocols and chemicals allowing for reproducibility. 
However, very often biological specimens such as strains, cultures or cultivars are often not available. 
These cases of a lack of reproducibility will not be covered here.

## Where do we stand in regards to reproducibility?

In an effort to understand where as a discipline we stand as plant pathologists in regards to open science and reproducible research, we surveyed a broad selection of articles to represent a broad swathe of publications to evaluate our status.
We randomly selected 300 articles published from 2012 to 2018 across 21 journals (Table \@ref(tab:journals)) and assigned them scores reflecting their openness and reproducibility.
The journals represented both fundamental and applied areas as well as pathogen or other group-specific publications (see appendix for details on the methodology).

Forty-eight articles scored 0% out of a possible 100% for reproducibility.
The mean value was 31.2% and the median value was 33.3%.
One article scored 100% or was classed as fully reproducible.
No journals surveyed were completely closed, all offered at least an option for open access but some were completely open.The majority of the articles were classed as fundamental, 160, with the remaining 138 classed as applied.

Most articles did not make any computational methods available in any fashion with two, 0.007%, classing as '3', which was the highest score available (Figure \@ref(fig:articleScores)i).
Two hundred eighty-seven articles, 96%, were classed as '0' and nine articles, 0.03%, appeared to not use any computational methods.

More articles did a good job of using software that was reasonably available to anyone with 206 scoring '2' or '3' (Figure  \@ref(fig:articleScores)ii).

Software citations were also reasonably well reported in the literature with 120 scoring '2' where the main software was cited with version numbers and 57 scoring '3' where full citations including R packages or the SAS PROC used were cited where appropriate (Figure \@ref(fig:articleScores)iii).

However, data availability was mostly not available with 243 or 81.5% scoring 0' where the data was not available or mentioned in the article.
However, more articles, 29 or 9.7%, scored '3' than '1' or '2' combined, with two articles not producing shareable data (Figure \@ref(fig:articleScores)iv).

The most frequently cited software was SAS.
The remainder were mostly specialised software except for Microsoft Excel, tied for 9th (Table \@ref(tab:software-used)).

Unfortunately using our scoring method, most articles scored poorly for reproducibility or openness.
Most journals in our discipline fail to mention the availability of the computational methods to support the article as published.

The software availability score was better as was the citing of software and versions used.
SAS remains firmly entrenched in the discipline as the choice of software for statistical analysis followed by SPSS.
It is interesting to note however, that with the popularity of Python that it does not even appear in our survey results whereas R was the fourth most commonly used software.

## Are there good examples in the field?

While the articles that were surveyed did not score well, we are aware of and would like to highlight that there are several good examples from the plant pathology discipline that can be used as exemplars.

There are different ways to make research more open and reproducible, which can be thought of as levels of reproducibility.
The first level is including tables of raw data or code with the paper as a supplemental file.
This is suitable for studies that may have a small data set or simple analysis or for demonstrations purposes as Madden _et al._ [-@Madden2015] demonstrate in their discussion regarding the use of $P$ values in statistical analyses where they supply an _e-**X**tra_\* with reproducible examples for readers to refer to.
This allows the reader to quickly view the extra materials and a DOI is assigned as a part of the article itself.

At the second level, Lehner _et al._ [-@Lehner2017] used GitHub to host a code repository of their research compendium website with data and a reproducible report that explains in detail all steps of the analysis and the R code for conducting a meta-analysis for assessing heterogeneity in the relationship between white mold incidence and soybean yield and between incidence and soybean yield ([https://emdelponte.github.io/paper-white-mold-meta-analysis/](https://emdelponte.github.io/paper-white-mold-meta-analysis/)).
The website clearly demonstrates the analysis to readers and uses R so that anyone can easily replicate the work.
Likewise, Duku _et al._ [-@Duku2016] provide scripts for models, data and code for graphs, via a website hosted by GitHub, [http://adamhsparks.github.io/MICCORDEA/](http://adamhsparks.github.io/MICCORDEA/), that can be used to replicate their study modelling the effects of climate change on rice bacterial blight and rice leaf blast in Tanzania. 
A final example is provided by Carleson et al. [-@carleson2019population] who hosted the code for reproducing a population genomic analysis of  _Phytophthora plurivora_ on github, while providing all data on OSF. 

The third level is the use of proper data repositories such as Github, FigShare, Zenodo or OSF.io among others, allow for the deposition and updating of code, figures, data preprints or any other materials that support the article itself while providing a DOI and citation for a standalone citation.
Sparks _et al._ [-@Sparks2011; -@Sparks2014] used FigShare to provide models, data and code, [@Sparks2016] necessary to replicate model development and the subsequent study on the effects of climate change on potato late blight.
Similarly, Carleson et al. [-@carleson2019population] provided all data on OSF [-@Carleson_Grünwald_2019]. 

The fourth level includes packaging the analysis as a software package, _e.g._, R, Julia or Python package at a minimum, with the use of continuous integration and possibly a software container like Docker as Khaliq _et al._ [@Khaliq2020] have done.
Their research compendium provides a Docker instance, DOI and full R package that enable readers to fully replicate their analysis of _Ascochyta rabiei_ conidia dispersal in chickpea using the data collected and stepping through other points where weather data were investigated and various models were fit before deciding on the best fit and recreate any figures as published in the article [@Khaliq2020a].
When this level is employed, tools such as Binder, [https://mybinder.org](https://mybinder.org), can be used that allow readers and reviewers to launch an interactive session in their web browser and interact with the data and rerun the analysis in an RStudio instance or Jupyter notebooks as Mioroni _et al._ used [-@Miorini2018, [-@Miorini2019]].
Kamvar _et al._ [-@Kamvar2015] took a slightly different approach by including all files necessary for the analysis and most output files in a repository [@Kamvar2014] that also included an installable R package that was used for the original analysis.
While there are many other methods, these two approaches illustrate some of the best-practices where the data and other files were deposited in repositories with DOIs and reproducibility issues were addressed by using R packages to handle dependencies and other versioning issues making the work more portable.

## Best practices to enhance reproducible research

A general work flow for producing academic research involves clearly defining a research question, obtaining data for testing the hypothesis, summarizing/analyzing and presenting data and results, and writing the manuscript.
Here we defined three levels of reproducibility which are also related to the evolution of computational methods and reproducible practices (Figure \@ref(fig:workflow-dia)).

The first level of reproducibility involves openly available research materials such as isolates or strains and/or nucleic acid sequences in public collections and citations for methods used.
A second level involves providing raw data and code as binary files (PDF or other non-text files) in supplemental materials, which do not allow prompt access to the data and running the code because of use of expensive commercial software or a paywall.
The highest level includes efforts to annotate structured raw data (FAIR -- Findable, Accessible, Interoperable and Re-usable) [@Wilkinson2016] and fully document the analysis using open source code which are deposited in public repositories and can be run by anyone following the download of data and code.
The first level, as reported, is an essential step that is not substituted by the other practices and eventually researchers fail to provide sufficient description or correct citations.
In the next section we present standards and tools that can be used to ensure reproducibility.

When making your science more open and reproducible, methods, software used (this includes items such as R, Julia or Python packages that were directly used in the analysis or production of the paper, etc.) should be cited properly. 
Deposit and annotate biological materials with an herbarium or other repository.
Provide a full description of all equipment used, _e.g._, 'a Spectrum Technologies Watchdog 2700 weather station was used to record wind speed, direction, rainfall, temperature and relative humidity at one-hour intervals.'
This allows end-users to identify what was used and identify the methods used more accurately.
Just as importantly, this acknowledges the contributions of others whose works were instrumental in your research.
This also helps ensure that you as a researcher can reconstruct what you have done since you will have good notes and documentation and be able to identify if something changes, _e.g._, a package version, what effect it had on your research.

The use of programming or scripting languages such as R, Julia, SAS or Python enables you as a scientist to keep very detailed records of what was computationally performed.
This is as opposed to using software such as spreadsheet programs like Excel, Google Sheets, Numbers, Calc or others which can be used for simple statistical analyses and visualization (but should not be) or other point-and-click software packages that do not enable you to keep an accurate record of the steps taken to import, format, visualise and analyse data.

Text files for saving small sets of data are preferable. 
Data that are saved in binary formats such as PDF files are difficult to reuse because they are not easily machine readable.
In many cases, data sets are small enough and curated in spreadsheets, which should be saved as a plain text file, _e.g._, comma separated (CSV) or tab separated (TSV) files.
This also helps ensure that the data are reusable.
Larger data sets may warrant the use of a proper database like GenBank, MariaDB or PostgreSQL which provide users with several benefits but two important benefits to mention here are (1) data redundancy, ensuring no records are duplicated and (2) data consistency, ensuring that all records in a data set are recorded in the same format for every observation.
While personal databases may offer many advantages, the trade-off is that they are more complex to set up and administer, especially for a small data set. 
Databases such a GenBank are preferable for molecular data ensuring data integrity and machine readability. 

Ideally, once the data are complete, best practices for keeping your data as you perform your work include treating the raw data as read-only and using file permissions to prevent changes to the raw data files.
It should be noted, that the use of a database management system also allows for both of these at the expense of added complexity.
Saving files in proprietary formats such as .xls(x) can also lead to issues in the future when opening using newer (or older) software versions.
Unexpected changes to values in the data [@Ziemann2016] may also occur when using proprietary formats.

If steps are followed to make the data FAIR then it will be readable by humans and machines alike and this will help support discoveries and support further research. 
In turn, sharing data will lead to new citations for your work as others discover and use it.
To make your data the most widely discoverable and usable, ensure that it has a persistent identifier.
A digital object identifier (DOI) is the most common ([https://www.doi.org/](https://www.doi.org/)) but the Handle.Net Registry (HNR) ([https://handle.net/](https://handle.net/)) is also an option.
There are different options for generating a DOI for your data and other materials.
FigShare, Zenodo and OSF all offer persistent archives along with a services to generate a DOI for your materials. 
The use of a persistent identifier works to ensure that even if the data are moved, they can still be located using that unique identifier.
For more on FAIR data, visit Go-Fair [https://www.go-fair.org/fair-principles/](https://www.go-fair.org/fair-principles/).

Once you have determined how to best manage your source code and the data sets for analysis the next step is to consider how to share your data.
Providers such as FigShare, Dataverse, OSF.io (the Open Science Framework) and Zenodo allow for you to deposit your data and generate a DOI for sharing your project once you are finished with it.

Other providers exist that allow for you to not only track changes but also to share the data openly, these include GitHub, Gitlab and Bitbucket.
GitHub is arguably the most popular and widely used software development platform currently.
Data that is encoded in CSV or other plain text formats can easily be deposited in a repository along with code for analysis to enable tracking of changes and other users to download and replicate the work.

We would advise against the practice of depositing data on a laboratory website or a site such as GitHub only as these are not an optimal way to preserve and share your work over the long term. 
Doing either of these leaves the work in an unstable state where future users may be unable to access the work as they are fraught with link-rot and other issues.
It is a best practice to always ensure that you have deposited the data with a provider such as GenBank, Zenodo, FigShare or OSF and generated a DOI for the materials to help ensure continued accessibility.
Many of these providers provide rather easy ways to link the project with a software development repository to help ensure that the data are available in perpetuity.
If readers are uncertain, we suggest to also consult with local librarians about possible resources.
Most universities and other research-focused workplaces provide a facility for staff to deposit papers and other academic materials, but this may extend to software development repositories as well.
The important aspect is that data once deposited cannot be modified any more. 

In this work we have evaluated the state of reproducibility in the plant pathology literature and presented suggestions for areas of improvement.
One of the most important messages that we would like to share is that there is more to the work than just the paper.
Sharing materials detailing the analysis that was performed and documenting the data provide citable products and enhance the manuscript providing the reader with a richer set of information with which to understand the work that was performed.
This open sharing of code and data often leads to greater impact as work is cited if resources such as code or data are reused.
With mandates for openly sharing data becoming more common with funding agencies, we can and should embrace this and move the discipline forward and have a greater impact with our work.

# Appendix

## Article selection

We randomly selected 300 articles published from 2012 to 2018 across 21 journals (Table \@ref(tab:journals)) and assigned them scores reflecting their openness and reproducibility.
The journals represented both fundamental and applied areas as well as pathogen or other group-specific publications.
A list of randomly selected pages was assigned to a randomized list of the 21 journals where the page number fell within an article for the given journal.
In cases where an article was not suitable, _e.g._, a review or otherwise not related to plant pathology, the next article was selected until a suitable article was found.
Notes regarding the selection of articles can be found in the file, "Reproducibility_in_plant_pathology_notes.ods", available in this paper's repository in `/inst/extdata'.
The pages list was numbered from page one and went to page 150.
This was done since some journals restart their numbering with each issue and also ensures that the journal is more likely to have a page number corresponding to the randomly generated value.
This also assumes that there is no effect or bias on reproducibility based on the time of year that an article was published since most journals start with page number one at the beginning of the year.

## Scoring criteria

Four individuals were assigned to rate a randomised list of journal articles using scoring criteria devised for the purposes of this research (Table \@ref(tab:scoring)).

Each journal was classified as to whether they were completely open (TRUE), behind a paywall (FALSE) or a combination (BOTH).
The five-year impact factor for 2018 for each journal was retrieved from InCites Journal Citation Reports, Clarivate Analytics and entered in a separate sheet in the Google sheets file.
This was downloaded, saved as an Open Document Spreadsheet (ODS) file and is left-joined with the article notes using a `left_join()` from the 'dplyr' package [@Wickham2020] for analysis when the `import_notes()` command is executed.

Articles were classified as fundamental or applied and a note was made whether the article's primary focus was using or developing molecular techniques (TRUE/FALSE).
This classification was based on the evaluator's judgement of the article.

Where possible, the software used in the preparation of the publication was recorded in the notes if it was cited or otherwise specified in the article text.

Values for software packages were checked for spelling consistency and corrections were made manually where necessary.
When working with these data in R, all strings of software character values were converted to fully upper-case to standardise the capitalisation and alleviate any issues with capitalisation used between evaluators.
A custom R function, `import_notes()`, was written to import the data, format the columns properly and calculate the overall reproducibility score in R [@RCT2021].

A total reproducibility score was calculated as the per cent of total possible for a given paper.
However, some papers did not use specialized software therefore they were not scored for this category.

We recognise that the scoring methods were developed by the authors specifically for this exercise and are not entirely subjective.
We have based our scoring decisions on our own experiences in the field of plant pathology.
While many software packages may be widely used or "freely" available if one has an institutional license, these types of software often hinder partners and interested parties that do not have these luxuries.
Thus, the emphasis was given to using open source alternatives where possible over commercially available, non-free software packages.
The view of the authors is one that this is something to be encouraged as it allows the broadest possible audience to benefit from the work at the lowest possible cost.

## Statistical analysis

The statistical analysis was performed using R version 4.1.0 [@RCT2021] on an Apple MacBook Air (M1, 2020).
Bayesian mixed-effect models were fit using the contributed package 'brms' [@Burkner2018] to evaluate the fixed effects of the publication (journal title), article class, molecular focus, five-year journal impact factor, journal open access policy on the reproducibility index and year to test for changes over time in the reproducibility index.

Relatively flat priors were selected with a mean of 0 and a standard deviation of 10 for the fixed effects.

The random effects were year and assignee for all models excepting the model which tested for year effects.
In this model, the random effects were the publication title and assignee.

Models were fit using the `hurdle_gamma()` family function due to zero-inflated continuous data and the assumption that a zero value would only come from one process, the scoring.
Each model was run for 3000 iterations in four chains.

The `adapt_delta()` and `max_treedepth()` values were adjusted as necessary on a case-by-case basis for each of the models to ensure that the chains mixed well.

Model finesses were evaluated using model summaries and diagnostic plots from 'brms' [@Burkner2018] and posterior fits using `pp_check()` from the contributed R package 'bayesplot' [@Gabry2019].

Probability of direction or maximum probability of effect was plotted, `pd()`, and a test for practical equivalence, `equivalence_test()`, was performed for each model using the contributed R package 'bayestestR' [@Makowski2019].

# Authors' contributions

A.H.S, E.DP., Z.F. and N.J.G. conceived of the presented idea and scoring system.
N.J.G. provided five-year impact factors.
A.H.S., E.DP., Z.F. and K.dSA. evaluated articles for scoring.
A.H.S. designed the computational framework for analysis and created the research compendium with feedback from E.DP. and Z.F.
A.H.S. wrote the manuscript in consultation with E.DP.
<!-- All authors provided critical feedback and helped contribute to the final version of the manuscript. -->

# Data availability statement

The raw data for this work are documented and available from DOI [https://doi.org/10.5281/zenodo.4941722](10.5281/zenodo.4941722).

# Code availability statement

All code used in the analyses and data visualization and associated materials have been made available as research compendium available from DOI [https://doi.org/10.5281/zenodo.1250664](10.5281/zenodo.1250664).
A webpage version of the compendium is available from <https://openplantpathology.github.io/Reproducibility_in_Plant_Pathology/>.

# Acknowledgements

\newpage

# References

<!-- The following line ensures the references appear here for the MS Word or HTML output files, rather than right at the end of the document (this will not work for PDF files):  -->

::: {#refs}
:::

\newpage

# Tables

```{r journals, echo=FALSE, message=FALSE, warning=FALSE}
import_notes() %>%
  tabyl(journal) %>%
  select(-percent) %>%
  rename(Journal = journal) %>%
  flextable() %>%
  colformat_num(na_str = "NA") %>%
  autofit() %>%
  set_caption(
    "Journal titles selected for inclusion and the number (n) of articles from each journal that were evaluated."
  )
```

\newpage

```{r software-used, echo=FALSE, message=FALSE, warning=FALSE}
rrpp_software <-
  import_notes() %>%
  transform(software_used = strsplit(software_used, ",")) %>%
  unnest(software_used) %>%
  mutate(software_used = trimws(software_used)) %>%
  mutate(software_used = toupper(software_used)) # convert all to uppercase to standardise

rrpp_software %>%
  tabyl(software_used) %>%
  adorn_pct_formatting() %>%
  select(c(-percent, -valid_percent)) %>%
  rename(`Software Used` = software_used) %>%
  arrange(-n) %>%
  slice(seq_len(14)) %>%
  drop_na() %>%
  flextable() %>%
  autofit() %>%
  set_caption(
    "The top ten sofware programs that were found to be used in the papers that were surveyed."
  )
```

\newpage

```{r scoring, echo=FALSE, message=FALSE, warning=FALSE}
scoring_criteria %>%
  rename(`Index Value` = Index.Value) %>%
  flextable() %>%
  colformat_num(na_str = "NA") %>%
  set_table_properties(layout = "autofit") %>%
  set_caption(
    "Articles were scored in four areas for reproducibility on a scale of '0' to '3' by four independent assessors to rank articles in each of four areas, 'Computational methods availability', 'Data availability', 'Software availability' and 'Software citation'.")
```

\newpage

# Figures

```{r articleScores, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Aggregated article scores for each of the four categories, (i) shows computational methods availability 0 - not available or not mentioned to 3 - freely available online to anonymous users. (ii) presents 'Software availability', where '0' was 'Not available or not mentioned in the publication'; '1' was 'Expensive proprietary software that only institutions would typically purchase'; '2' was 'Used proprietary software that most individuals can afford (e.g., Excel) or SAS that is now available free for use with a university login'; and '3' was 'Used entirely open source and free software (e.g., R, Julia, Python)'; 'NA' indicates that no specialised software was necessary or used. (iii) displays 'Software citated', where '0' was 'Not available or not mentioned in the publication'; '1' was 'Software mentioned by name only'; '2' was 'Software cited with version number'; and '3' was 'All software components (SAS PROCs, R, Julia or Python packages, etc.) cited'; 'NA' indicates that no specialised software was necessary or used. (iv) shows 'Data availability', where '0' was 'Not available or not mentioned in the publication'; '1' was 'Available upon request to the author; '2' was 'Online, but inconvenient or non-permanent (e.g., login needed, paywall, FTP server, personal lab website that may disappear, or may have already disappeared)'; and '3' was 'Freely available online to anonymous users for foreseeable future (e.g., archived using Zenodo, dataverse or university library or some other proper archiving system)'; 'NA' indicates that no data were generated, e.g. a methods paper.", cache=FALSE, fig.env='figure', out.width=0, out.height=0}

rrpp <- import_notes()
theme_set(theme_classic())
i <- ggplot(rrpp, aes(y = as.factor(comp_mthds_avail))) +
  geom_bar() +
  ylab("Score") +
  xlab(NULL)

ii <- ggplot(rrpp, aes(y = software_avail)) +
  geom_bar() +
  xlab(NULL) +
  ylab(NULL)

iii <- ggplot(rrpp, aes(y = software_cite)) +
  geom_bar() +
  ylab("Score")

iv <- ggplot(rrpp, aes(y = data_avail)) +
  geom_bar() +
  ylab(NULL)

# put them all together
p <- (i | ii) / (iii | iv)
p <- p + plot_annotation(tag_levels = "i")
ggsave(filename = "../figures/Sparks et al Figure 2.eps",
       plot = p,
       device = "eps",
       width = 6,
       height = 6,
       units = "in")
extrafont::embed_fonts(
   file = ("../figures/Sparks et al Figure 2.eps"),
   outfile = ("figures/Sparks et al Figure 2.eps"),
   options = "-dEPSCrop"
)
knitr::include_graphics(path = "../figures/Sparks et al Figure 2.eps")
```

\newpage

```{r workflow-dia, echo=FALSE, message=FALSE, warning=FALSE, fig.cap='An example of an open and reproducible research workflow. Starting with the question, determine the methodology, describe it, make it available and cite it. Data are used in analysis and any binary files or code are made available as supplements to the manuscript. Source code and raw data are made available in a public repository, preferably with version control for tracking changes through time, and a DOI for final released products.', cache=FALSE, fig.env='figure', out.width=0, out.height=0}

p <- export_svg(
  grViz(
    "digraph Fig1 {
    graph [fontsize = 8, nodesep=0.5]
  
    node [shape = box, style = filled, fillcolor = grey99, width = 1.2,fontname = Helvetica]
    Question; Methodology; Data; Analysis; Manuscript;
  
    node [shape = box, style = filled, fillcolor = grey90, width = 1.2,fontname = Helvetica]
    Description; Availability; Citation
  
    node [shape = box, style = filled, fillcolor = grey80, width = 1.2]
    'Binary Code'; 'Binary File'; Supplement
  
    node [shape = box, style = filled, fillcolor = grey70, width = 1.2]
    'Source Code'; 'Data File'; 'Public Repository';
  
    Question->Methodology->Data->Analysis
    Analysis->Manuscript
    Methodology->Description
    Methodology->Citation
    Methodology->Availability
    Manuscript->Supplement
    Data->'Data File'->'Public Repository'
    Analysis->'Binary Code'->Supplement
    Analysis->'Source Code'->'Public Repository'
    Data->'Binary File'->Supplement
  }",
  width = 1000,
  height = 750
))
rsvg_ps(svg = charToRaw(p), file = "../figures/Sparks et al Figure 1.eps")
extrafont::embed_fonts(
   file = ("../figures/Sparks et al Figure 1.eps"),
   outfile = ("figures/Sparks et al Figure 1.eps"),
   options = "-dEPSCrop"
)
knitr::include_graphics(path = "../figures/Sparks et al Figure 1.eps")
```

\newpage

# Colophon

This report was generated on $`r Sys.time()`$ using the following computational environment and dependencies:

```{r colophon, cache = FALSE}
# which R packages and versions?
sessioninfo::session_info()
```

The current Git commit details are:

```{r}
# what commit is this file at? You may need to change the path value
# if your Rmd is not in analysis/paper/
git2r::repository("../..")
```
