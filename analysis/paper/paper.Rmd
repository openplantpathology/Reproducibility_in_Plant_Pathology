---
title: "Open Science, Open Plant Pathology"
author:
  - Adam H. Sparks
  - Emerson Del Ponte
  - Sydney Everhart
  - Zachary Foster
  - Niklaus J Grünwald
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
    bookdown::word_document2:
      fig_caption: yes
      reference_docx: "../templates/template.docx" # Insert path for the DOCX file
bibliography: references.bib
csl: "../templates/phytopathology.csl" # Insert path for the bib-style
abstract: |
  Open research practices have been highlighted extensively during the last ten years in many fields of study as essential standards needed to promote transparency and reproducibility of scientific results. In fact, scientific claims can only be evaluated based on how protocols, materials, equipment and methods were described; data were collected and prepared; and analyses conducted. Openly sharing protocols, data and computational code are central for current scholarly dissemination and communication, but in many fields, including plant pathology, adoption of these practices has been slow. We randomly selected 200 articles published from 2012 to 2016 across 21 journals representative of the pathology discipline, and assigned them scores reflecting their openness and reproducibility. We found that most of the articles were not very open, failing to share data or code in a meaningful way. We propose that using open source tools for producing open and reproducible work and analysis is advantageous, benefiting not just readers, but the authors as well and provide ideas and tools to promote open, reproducible research practices among plant pathologists.
keywords: |
  open science; open data; reproducible research; FAIR data
highlights: |
  Open research practices are highlighted extensively in the broader scientific community. Our survey of 200 randomly selected articles from 2012 to 2016 indicates that we have work to do in this area as plant pathologists with most articles scoring low in openness and reproducibility.
---

```{r, setup, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  echo = FALSE,
  comment = "#>",
  fig.path = "../figures/"
)

library(Reproducibility.in.Plant.Pathology)
```

# Introduction

Modern plant pathological research has many facets given the array of disciplines and sub-disciplines currently involved. Collectively, they contribute to increase our basic and applied knowledge on several aspects of pathogen biology and disease development to ultimately improve plant disease management. Scientific research in the field varies from the purely observational or descriptive nature to inferential based on experimental or simulation-derived data sets. Whatever the case, research findings are verifiable based on how much of the research materials, processes and outcomes are made available beyond what is reported in the scientific article. These include biological materials (strains), nucleic/protein sequences, experimental and simulated raw data annotations, drawings and photographs and statistical analysis code among other materials and data. That is, open science leads to reproducibility and replicability.

## Definitions

In order for us to easily discuss the topic, we first must define what we mean so that we may clearly communicate. Many of the terms used in this area have varying definitions that may or may not agree with each other. For instance, reproducible research is recently hightlighted by many authors [@Baker2016a; @Iqbal2016; @Nature2016; @Weissgerber2016; @Brunsdon2015; @Sweedler2015; @Fitzjohn2014; @Ioannidis2014; @Fidler2013; @Stodden2013] as an important issue. This has resulted in attepmts to define "reproducibility", _e.g._ [@Patil2016], therefore the following definitions will be used for these terms throughout this paper.

### Repeatability

The International Union of Pure and Applied Chemistry defines repeatability in the "Compendium of Chemical Terminology Gold Book" [@mcnaughtcompendium],

> "The closeness of agreement between independent results obtained with the same method on identical test material, under the same conditions (same operator, same apparatus, same laboratory and after short intervals of time). The measure of repeatability is the standard deviation qualified with the term: `repeatability' as repeatability standard deviation. In some contexts repeatability may be defined as the value below which the absolute difference between two single test results obtained under the above conditions, may be expected to lie with a specified probability."

### Reproducibility

The International Union of Pure and Applied Chemistry defines reproducibility in the "Compendium of Chemical Terminology Gold Book" [@mcnaughtcompendium],

>"The closeness of agreement between independent results obtained with the same method on identical test material but under different conditions (different operators, different apparatus, different laboratories and/or after different intervals of time). The measure of reproducibility is the standard deviation qualified with the term 'reproducibility' as reproducibility standard deviation. In some contexts reproducibility may be defined as the value below which the absolute difference between two single test results on identical material obtained under the above conditions, may be expected to lie with a specified probability. Note that a complete statement of reproducibility requires specification of the experimental conditions which differ."

### Replicability

XXXX

### Open Science

Open science has become highlighted lately with many donors expecting data to be available (XXXX) and other scientists interested in sharing and collaborating more widely (XXXX).

# A General Workflow

A general workflow for producing academic research involves clearly defining a research question, obtaining data for testing the hypothesis, summarizing/analyzing and presenting data and results, and writing the manuscript. Here we defined three levels of reproducibility which are also related with the evolution of computational methods and reproducible practices \@ref(fig:workflow-dia).

A first level of reproducibility involves openly available research materials such as strains and/or nucleic acid sequences in public collections and citations for methods used. A second level involves providing raw data and code as binary files (PDF or other non-text file) in supplemental materials, which do not allow prompt access to the data and running the code because of use of expensive commercial software or a pay-wall. The highest level includes efforts to annotate structured raw data (FAIR -- Findable, Accessible, Interoperable and Re-usable) and fully document the analysis using open source code which are deposited in public repositories and can be run by anyone following download of data and code. The first level, as reported, is an essential step that is not substituted by the other practices and eventually researchers fail to provide sufficient description or correct citations. In the next section we present standards and tools that can be used to ensure reproducibility.

### Methods

When making your science more open and reproducible, methods, software used (including things like R packages), etc. should be cited properly. This allows end users to identify what was used and identify the methods used more accurately. Just as importantly, this acknowledges the contributions of others whose works were instrumental in your research. This also helps ensure that you as a researcher are able to reconstruct what you have done since you will have good notes and documentation and be able to identify if something changes, _e.g._ a package version, what effect it had on your research.

- deposit and annotate biological materials

- provide full description for equipments, etc.

- rOpenSci Reproducible Research: http://ropensci.github.io/reproducibility-guide/sections/introduction/

### Data

- Data formatting (flat files; use Comma Chameleon, Table Tool, others?)
- Data annotation
- Data storage (don't edit raw data files; use file permissions to prevent changes to raw data files, use data bases where possible and appropriate; etc.)

### Source Code

- The problem of commercial software and mouse-based routines
- Why to avoid binary files as supplements?
- Writing and documenting using open source software
- Availability in public repositories

### Repository
- Using GitHub, Gitlab, Bitbucket for code (and small data?)
- Using Figshare, Dataverse or some other data repository for large data
- Zenodo or OSF to generate DOIs
- Why not use a lab website (DOIs, other reasons)
- Most ideally, deposit in university library or other proper long-term data archival service


# Status in Plant Pathology

* Madden et al. [-@Madden2015] supply an *e-**X**tra*\* with reproducible examples
for readers.
* Duku et al. [-@Duku2016] provide models, data and code, (http://adamhsparks.github.io/MICCORDEA/) necessary to
replicate the entire study modelling the effects of climate change on rice
bacterial blight and rice leaf blast in Tanzania.
* Sparks et al. [-@Sparks2011; -@Sparks2014] provide models, data and code, (http://adamhsparks.github.io/Global-Late-Blight-MetaModelling/)
necessary to replicate model development and the subsequent the study on the effects of climate change on potato late blight.
* Del Ponte provides data and a reproducible report that explain in details all steps of the analysis and the R codes for conducting a meta-analysis for assessing heterogeneity in relationship between white mold incidence and soybean yield and between incidence and soybean tied.
* Example from Grünwald lab: 
  - paper http://apsjournals.apsnet.org/doi/full/10.1094/PHYTO-12-14-0350-FI
  - github repo https://github.com/grunwaldlab/Sudden_Oak_Death_in_Oregon_Forests
* Other examples from plant pathology providing e-Xtras or supplemental material

Twenty-one plant pathology discipline journals were selected by the authors as representations of discipline-based journals target by the plant pathology research community. Among them, both fundamental and/or applied as well as journals covering specific group of pathogens/plants or broad areas were included. Two hundred articles were randomly selected from issues published from 2012 to 2016. A list of randomly selected pages was assigned to a randomized list of the 21 journals [@Sparks2017] where the page number fell within an article for the given journal. In cases where an article was not suitable, _e.g._, a review or otherwise not related to plant pathology, the next article was selected until a suitable article was found. Notes regarding the selection of articles can be found in the file, XXXX, available in this paper's repository. The pages list was numbered from page one and went to 150. This was done since some journals restart their numbering with each issue and also ensures that the journal is more likely to have a page number corresponding to the randomly generated value. This also assumes that there is no effect or bias on reproducibility based on the time of year that an article was published, since most journals start with page number one at the beginning of the year. The list of journals was saved as a comma separated value (CSV) file and imported into R [@R2018]. 

# Discussion

# Acknowledgements

<!-- The following line inserts a page break when the output is MS Word. For page breaks in PDF, use \newpage on its own line.  -->
##### pagebreak

# References 
<!-- The following line ensures the references appear here for the MS Word or HTML output files, rather than right at the end of the document (this will not work for PDF files):  -->
<div id="refs"></div>

##### pagebreak

<!-- The following line inserts a page break when the output is MS Word. For page breaks in PDF, use \newpage on its own line.  -->
##### pagebreak

# Figures

```{r workflow-dia, echo=FALSE, message=FALSE, warning=FALSE, fig.cap='An open and reproducible research workflow.', cache=FALSE, fig.env='figure'}
library(DiagrammeR)
library(DiagrammeRsvg)
library(rsvg)

p <- export_svg(
  grViz(
    "digraph Fig1 {
    graph [fontsize = 8, nodesep=0.5]
  
    node [shape = box, style = filled, fillcolor = grey99, width = 1.2,fontname = Helvetica]
    Question; Methodology; Data; Analysis; Manuscript;
  
    node [shape = box, style = filled, fillcolor = grey90, width = 1.2,fontname = Helvetica]
    Description; Availability; Citation
  
    node [shape = box, style = filled, fillcolor = grey80, width = 1.2]
    'Binary Code'; 'Binary File'; Supplement
  
    node [shape = box, style = filled, fillcolor = grey70, width = 1.2]
    'Source Code'; 'Data File'; 'Public Repository';
  
    Question->Methodology->Data->Analysis
    Analysis->Manuscript
    Methodology->Description
    Methodology->Citation
    Methodology->Availability
    Manuscript->Supplement
    Data->'Data File'->'Public Repository'
    Analysis->'Binary Code'->Supplement
    Analysis->'Source Code'->'Public Repository'
    Data->'Binary File'->Supplement
  }",
  width = 1000,
  height = 750
))
rsvg_pdf(charToRaw(p), "Figure 1.pdf")

knitr::include_graphics("Figure 1.pdf")
```

<!-- The following line inserts a page break when the output is MS Word. For page breaks in PDF, use \newpage on its own line.  -->
##### pagebreak

# Colophon

This report was generated on `r Sys.time()` using the following computational environment and dependencies: 

```{r colophon, cache = FALSE}
# which R packages and versions?
sessionInfo()
```

The current Git commit details are:

```{r}
# what commit is this file at? You may need to change the path value
# if your Rmd is not in analysis/paper/
git2r::repository("../..")
```
