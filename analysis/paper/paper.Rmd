---
title: "Openness and computational reproducibility in plant pathology: where do we stand and a way forward"
author:
  - Adam H. Sparks:
      email: Adam.Sparks@dpird.wa.gov.au
      institute: DPIRD, USQ
      correspondence: true
  - Emerson M. Del Ponte:
      institute: UFV
  - Kaique S. Alves:
      institute: UFV
  - Zachary S. L. Foster:
      institute: OSU
  - Niklaus J. Grünwald:
      institute: ARS
institute:
  - DPIRD: Department of Primary Industries and Regional Development, Farming Systems Innovation, Perth WA 6000, Australia
  - USQ: University of Southern Queensland, Centre for Crop Health, Toowoomba Qld 4350, Australia
  - UFV: Departmento de Fitopatologia, Universidade Federal de Viçosa, Brazil
  - OSU: Department of Botany and Plant Pathology, Oregon State University, Corvallis OR 97331, USA
  - ARS: Horticultural Crops Research Unit, USDA Agricultural Research Service, Corvallis OR 97330, USA
output: 
  bookdown::word_document2:
     fig_caption: yes
     number_sections: FALSE
     reference_docx: "../templates/phytopath_template.docx"
     pandoc_args:
       - '--lua-filter=scholarly-metadata.lua'
       - '--lua-filter=author-info-blocks.lua'
       - '--lua-filter=pagebreak.lua'
bibliography: references.bib
link-citations: no
csl: "../templates/american-phytopathological-society.csl"
---

```{r, setup, echo = FALSE, message=FALSE}
packages <- c(
  "DiagrammeR",
  "DiagrammeRsvg",
  "bayesplot",
  "bayestestR",
  "brms",
  "dplyr",
  "english",
  "extrafont",
  "flextable",
  "ggpubr",
  "grDevices",
  "here",
  "janitor",
  "knitr",
  "officer",
  "parameters",
  "patchwork",
  "posterior",
  "report",
  "rsvg",
  "tidyr",
  "tools"
)

# install packages if needed
install.packages(setdiff(packages,
                         rownames(installed.packages())),
                 repos = "http://cran.us.r-project.org",
                 dependencies = TRUE)

# load packages for use in knitting Rmd
invisible(lapply(packages,
                 library,
                 character.only = TRUE))

library("Reproducibility.in.Plant.Pathology")

rrpp <- import_notes()
ir <- import_interrater_scores()

opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  echo = FALSE,
  out.width = "100%",
  comment = "#>",
  dev = "cairo_ps",
  cache = FALSE
)

theme_set(theme_classic())

load(
  system.file(
    "extdata",
    "m_f1.Rda",
    package = "Reproducibility.in.Plant.Pathology",
    mustWork = TRUE
  )
)
load(
  system.file(
    "extdata",
    "m_f1_report.Rda",
    package = "Reproducibility.in.Plant.Pathology",
    mustWork = TRUE
  )
)
load(
  system.file(
    "extdata",
    "m_f2.Rda",
    package = "Reproducibility.in.Plant.Pathology",
    mustWork = TRUE
  )
)
load(
  system.file(
    "extdata",
    "m_f2_report.Rda",
    package = "Reproducibility.in.Plant.Pathology",
    mustWork = TRUE
  )
)

load(
  system.file(
    "extdata",
    "m_g1.Rda",
    package = "Reproducibility.in.Plant.Pathology",
    mustWork = TRUE
  )
)
load(
  system.file(
    "extdata",
    "m_g1_report.Rda",
    package = "Reproducibility.in.Plant.Pathology",
    mustWork = TRUE
  )
)
load(
  system.file(
    "extdata",
    "m_g2.Rda",
    package = "Reproducibility.in.Plant.Pathology",
    mustWork = TRUE
  )
)
load(
  system.file(
    "extdata",
    "m_g2_report.Rda",
    package = "Reproducibility.in.Plant.Pathology",
    mustWork = TRUE
  )
)

load(
  system.file(
    "extdata",
    "m_h1.Rda",
    package = "Reproducibility.in.Plant.Pathology",
    mustWork = TRUE
  )
)
load(
  system.file(
    "extdata",
    "m_h1_report.Rda",
    package = "Reproducibility.in.Plant.Pathology",
    mustWork = TRUE
  )
)
load(
  system.file(
    "extdata",
    "m_h2.Rda",
    package = "Reproducibility.in.Plant.Pathology",
    mustWork = TRUE
  )
)
load(
  system.file(
    "extdata",
    "m_h2_report.Rda",
    package = "Reproducibility.in.Plant.Pathology",
    mustWork = TRUE
  )
)
```

```{r fonts, include=FALSE, message=FALSE, eval=FALSE}
# Note 1: This chunk only needs to be run once on a computer to import fonts for
# use in R. This chunk makes .ttf fonts available to R for use and embedding in
# .eps figure outputs.
#
# Note 2: GhostScript needs to be installed at the system level for the PS files
# to be generated.  MacOS users can use `brew install ghostscript`.  Windows
# users should download and install. Linux users can use their package manager.
#
# Note 3: You may need to tell R where to find GhostScript's 'gs' executable.
# Set `R_GSCMD="/opt/homebrew/bin/gs"` in .Renviron for MacOS users using
# homebrew to install on a Mac M1.
# 
# Windows users can follow these directions:
# 1.	Go to the GhostScript website
#     <https://www.ghostscript.com/download/gsdnld.html>.
# 2.	Download the windows installer suitable for your machine
# 3.	Run the installer file which you downloaded and follow the prompts
# 4.	After running the installer click the windows "Start" button and type
#     "Edit environment variables for your account" and open
# 5.	In the tab 'Advanced' click the button at the bottom
#     'Environment Variables...'
# 6.	Under 'System variables' find the variable 'Path', select 'Path' and click
#     the 'Edit' button
# 7. 	Select a new line and copy the Ghostscript 'bin' folder location into the
#     field.
## 7.1	If you installed Ghostscript to the default folder location; then the
#     folder location will likely be "C:\Program Files\gs\gs9.52\bin", the
#     version number (9.52) may differ.
# 8.	Save and exit the environmental variables window
#
# This chunk is then run only if knitting on new computer that the files have
# not been generated

font_import()
```

**Keywords: Data Science, Computational Biology, Techniques**

\newpage

# Abstract {#abstract .unnumbered}

Open research practices have been highlighted extensively during the last ten years in many fields of scientific study as essential standards needed to promote transparency and reproducibility of scientific results.
Scientific claims can only be evaluated based on how protocols, materials, equipment and methods were described; data were collected and prepared; and analyzes were conducted.
Openly sharing protocols, data and computational code is central for current scholarly dissemination and communication, but in many fields, including plant pathology, adoption of these practices has been slow.
We randomly selected 450 articles published from 2012 to 2021 across 21 journals representative of the plant pathology discipline and assigned them scores reflecting their openness and computational reproducibility.
We found that most of the articles were not following protocols for open science and were failing to share data or code in a reproducible way.
We also propose that use of open-source tools facilitates computationally reproducible work and analyzes benefitting not just readers, but the authors as well.
Finally, we also provide ideas and tools to promote open, reproducible computational research practices among plant pathologists.

\newpage

# Introduction

Modern plant pathological research has many facets given the array of disciplines and sub-disciplines currently involved.
Collectively, they contribute to increasing our basic and applied knowledge of several aspects of pathogen biology and disease development to ultimately improve plant disease management.
Scientific research in the field varies from the purely observational or descriptive nature to inferential, based on experimental or simulation-derived data sets.
Whatever the case, research findings are verifiable based on how much of the research materials, processes and outcomes are made available beyond what is reported in the scientific article and the ability of others to make use of the methods and results.
These research findings include biological materials (host and pathogen genotypes), nucleic/protein sequences, experimental and simulated raw data annotations, drawings and photographs and statistical analysis code among other materials and data generated during the research activities.

Recently, open science has been highlighted with many funding agencies expecting data to be available upon the conclusion of the research project [@government_of_canada_2016; @vannoorden2017; @ARC2018], journals in the field promoting the sharing of data [@DelPonte2020], and other scientists interested in sharing and collaborating more widely [@Wald2010].

Reproducibility is one component under the umbrella of open science.
By proactively practicing open science, the work increases the chance to become more reproducible through the availability of data and code.
That is, open science leads to reproducibility and replicability.

For us to easily discuss the topic, we first must define what we mean so that we may clearly communicate.
Many of the terms used in this area have varying definitions that may or may not agree with each other.
For instance, reproducible research was recently highlighted by many authors [@Tiwari_2021; @Eckert2020; @Dienlin2020; @preeyanon2018; @Wallach2018; @Baker2016a; @Iqbal2016; @Nature2016; @Patil2016; @Weissgerber2016; @Brunsdon2015; @Sweedler2015; @Fitzjohn2014; @Ioannidis2014; @Fidler2013; @Stodden2013] as an important issue.

In the biological sciences it is not always possible to use identical test material or perhaps the time or resources are not available for full reproducibility, *e.g.*, field trials that span years and locations or complex glasshouse experiments.
Therefore, we will follow Peng's [-@Peng2009] definition that provides clear guidelines for a minimum standard of "reproducible research":

> "The replication of scientific findings using independent investigators, methods, data, equipment, and protocols has long been, and will continue to be, the standard by which scientific claims are evaluated. However, in many fields of study there are examples of scientific investigations that cannot be fully replicated because of a lack of time or resources. In such a situation, there is a need for a minimum standard that can fill the void between full replication and nothing. One candidate for this minimum standard is 'reproducible research', which requires that data sets and computer code be made available to others for verifying published results and conducting alternative analyzes".
> @Peng2009.
> *Reproducible research and Biostatistics*.
> Biostatistics, 10 (3): 405-408.

Therefore, our definition of reproducibility will be that the computer code and data are made freely available to others for verification and conducting alternate analyzes or for use in instructional purposes.
And that the software used are also easily obtained and preferably open-source to avoid licensing or other issues related to accessibility for end-users related to costs or non-standard file formats, etc.

Plant pathologists routinely provide information on protocols and chemicals allowing for reproducibility.
However, frequently biological specimens such as strains, cultures or cultivars are not available after publication.
These cases do constitute a lack of reproducibility but will not be covered here.

# Materials and Methods

## Article Selection and Evaluation Events

To understand where we stand as a discipline regarding open science and reproducible research, we surveyed a broad selection of articles to represent a wide swathe of publications to evaluate our collective status.
We hand-picked 21 journals which we felt represented research publications in the field of plant pathology (Table \@ref(tab:journals)) that encompassed a range of subject matter foci, applied and fundamental work, country of origin and ranking metrics, *e.g.*, quartile range or citation index, to represent the discipline.
The aim was to gather as complete an overview of the status of computational reproducibility in plant pathology journals as possible and avoid bias in the findings by skewing towards high-impact journals that may have a greater influence.
From those 21 journals we randomly selected 450 articles published from 2012 to 2021.
Using R [@RCT2022], two lists were created, the first was a list of the 21 journals and the second was a list of the evaluators that were evaluating the articles for reproducibility.
Initially there were four evaluators, later a fifth was added and the list was recreated.
There were three evaluation events that took place to increase the yearly coverage of the evaluations.
During each scoring event, each evaluator was randomly assigned 50 unique articles to evaluate during an evaluation event.

A list of randomly generated numbers representing page numbers from one to 150, sampled with replacement, was assigned to a randomized list of the 21 journals for each sampling event.
This was done because some journals restart their numbering with each issue and also ensures that the journal is more likely to have a page number corresponding to the randomly generated value.
This also assumes that there is no effect or bias on reproducibility based on the time of year that an article was published since most journals start with page number one at the beginning of the year.

Articles were then manually selected by visiting the journals' websites and selecting the articles within which the randomly assigned page numbers fell, *i.e.*, if the page number was 32, the article that started on page 28 and ended on page 35 was selected as it contained page 32.
In cases where an article was not suitable, *e.g.*, a review or otherwise not related to plant pathology or the randomly assigned numbers for that journal and year fell within the same article, the next article in that journal was selected until a suitable article was found.

Depending on the scoring event, three to five of this paper's authors were each assigned to rate a randomized list of journal articles using scoring criteria devised for the purposes of this research in each of the three different scoring events as their time allowed for their participation.
In the first event, three authors (AS, EDP, and ZF) evaluated a total of 200 unique articles, in the second event two authors evaluated a total of 100 unique articles (EDP, KA) and in the third round, five authors (AS, EDP, KA, ZF, NG) evaluated a total of 150 unique articles for a total of 450 articles.
Each article was evaluated by only one evaluator save five articles randomly selected for inter-rater repeatability analysis from the third and final evaluation event.

## Scoring Criteria

The five-year impact factor for 2022 for each journal was retrieved from InCites Journal Citation Reports, Clarivate Analytics and entered in a separate sheet in the Google sheets file.
This was downloaded, saved as an Open Document Spreadsheet (ODS) file for analysis.

Each article was rated on a 0 -- 3 scale for its data and code availability.
'Code availability' rated how easily and openly available the computational methods used in the article were.
Scores were assigned as '0' was 'Not available or not mentioned in the publication'; '1' was 'Available upon request to the author; '2' was 'Online, but inconvenient or non-permanent (e.g., login needed, paywall, FTP server, personal lab website that may disappear, or may have already disappeared)'; and '3' was 'Freely available online to anonymous users for foreseeable future (e.g., archived using Zenodo, dataverse or university library or some other proper archiving system)'; 'NA' indicates that no code was created to conduct the work that was detectable.
Second, the 'Data availability' evaluated how freely and openly available the data presented in the article were.
This was evaluated as '0' was 'Not available or not mentioned in the publication'; '1' was 'Available upon request to the author; '2' was 'Online, but inconvenient or non-permanent (e.g., login needed, paywall, FTP server, personal lab website that may disappear, or may have already disappeared)'; and '3' was 'Freely available online to anonymous users for foreseeable future (e.g., archived using Zenodo, dataverse or university library or some other proper archiving system)'; 'NA' indicates that no data were generated, e.g., a methods paper.

Where possible, the software used in conducting the research behind the publication was recorded in the notes when it was cited or otherwise specified in the article text.

## Data Cleaning

A custom function, `import_notes()`, was written to import the data and format the columns properly R [@RCT2022].
Values for software packages were checked for spelling consistency and corrections were made manually where necessary.
When working with these data in R, all strings of software character values were converted to fully upper-case to standardize the capitalization and alleviate any issues with capitalization used between evaluators.

## Statistical Analysis

The statistical analysis was performed using `r version$version.string` [@RCT2022] on an Apple MacBook Pro (13-inch, M1, 2020).

Inter-rater differences were evaluated using percent agreement and Fliess' Kappa [@DelPonte2019].

We fitted Bayesian logistic mixed models (estimated using MCMC sampling with 4 chains of 10000 iterations and a warm-up of 5000 and thinning of 1) using cumulative family function with a logit link for ordinal data using the contributed package 'brms' (version `r packageVersion("brms")`) [@Burkner2017; @Burkner2018; @Burkner2021].
The cumulative family allows for the probability of a given value is the probability of that value or any smaller value.
Priors were selected to be weakly informative and deemed suitable through using `pp_check()` to examine the the models' predictions based on priors only (using the parameter `sample_prior = only` in `brm()`).
Priors over all parameters were set as: normal (mean = 0.00, SD = 1.0) distributions for both fixed parameters and random effects.
The best fitting models were selected by using Widely Applicable Information Criterion (WAIC) and the expected log point-wise predictive density via `loo_compare()`.

Six models were applied, with half predicting the response of code availability and the other half predicting data availability.
Models 1 and 2 to predict two responses with publishing journal, as the journals’ abbreviation, the formula took the form: response ~ abbreviation (model 1: code_availability ~ abbreviation and model 2: data_availability ~ abbreviation) and the models included assignee as a random effect (formula: ~1 | assignee).
Models 3 and 4 to predict the two responses with year (formula: response ~ year) and the models included abbreviation and assignee as random effects (formula: list(~1 | abbreviation, ~1 | assignee)). 
Models 5 and 6 to predict the two responses with IF_5year the formula took the form: response ~ IF_5year and included assignee as random effect (formula: ~1 | assignee) without journal as a random effect to mitigate correlation between impact factor and the journal of publication.

The `adapt_delta()` and `max_treedepth()` values were adjusted as necessary on a case-by-case basis for each of the models to ensure a good model fit to the data and that the chains mixed well.

Model fitness' were evaluated using model summaries and diagnostic plots from 'brms' (version `r packageVersion("brms")`) and posterior fits using `pp_check()` (Figure \@ref(fig:posteriors)) from the contributed R package 'bayesplot' (version `r packageVersion("bayesplot")`) [@Gabry2019].

A test for practical equivalence, `equivalence_test()`, was performed for each model using the contributed R package 'bayestestR' (version `r packageVersion("bayestestR")`) [@Makowski2019] with the Region of Practical Equivalence (ROPE) set to -0.1 to 0.1 and confidence interval of 0.95.

Using the Sequential Effect eXistence and sIgnificance Testing (SEXIT) framework, the median of the posterior distribution and its 95% CI (Highest Density Interval), along the probability of direction (pd), the probability of significance and the probability of being large are reported.
The thresholds beyond which the effect is considered as significant (i.e., non-negligible) and large are 0.05 and 0.3 as suggested by @Makowski2019.
Convergence and stability of the Bayesian sampling was assessed using R-hat, which should be below 1.01 [@Vehtari2021] and Effective Sample Size (ESS), which should be greater than 1000 [@Burkner2017].

A full report of all parameters and model details are available in the Supplementary Materials generated using the contributed package 'report' (version `r packageVersion("report")`) [@Makowski2021].
Additionally, all methods are described in greater detail with the code necessary to reproduce the work in @Sparks2022.

# Results

## Inter-rater Agreement

All authors agreed on the five inter-rater article evaluations for the data availability, '0', not available for an inter-rater score of 100%.
However, one of the authors rated one article as 'NA' or not having any computational methods used rather than '0' as with the other four authors, giving the inter-rater percent agreement a score of 100% with the 'NA' value dropped from the code availability.

## Code and Data Sharing Findings

Most articles did not make any computational methods available in any fashion with `r length(which(rrpp$comp_mthds_avail == 3))` (`r round(length(which(rrpp$comp_mthds_avail == 3))/nrow(rrpp), 3) * 100`%) classified as '3', which was the highest score available (Figure \@ref(fig:articleScores)A).
`r gsub("(^[[:alpha:]])", "\\U\\1", as.character(english(length(which(rrpp$comp_mthds_avail == 0)))), perl = TRUE)` (`r round(length(which(rrpp$comp_mthds_avail == 0))/nrow(rrpp), 3) * 100`%) were classed as '0' and `r as.character(english(length(which(is.na(rrpp$comp_mthds_avail)))))` articles (`r round(length(which(is.na(rrpp$comp_mthds_avail)))/nrow(rrpp), 2) * 100`%) appeared to not use any computational methods.

Additionally, data that supported the articles were mostly unavailable with `r length(which(rrpp$data_avail == 0))` (`r round(length(which(rrpp$data_avail == 0))/nrow(rrpp), 2) * 100`%) scoring '0' where the data was not available or mentioned in the article.
However, more articles, `r length(which(rrpp$data_avail == 3))` (`r round(length(which(rrpp$data_avail == 3))/nrow(rrpp), 2) * 100`%), scored '3' than scored '1' or '2' combined, `r length(which(rrpp$data_avail == 2 | rrpp$data_avail == 1))` (`r round(length(which(rrpp$data_avail == 2 | rrpp$data_avail == 1))/nrow(rrpp), 2) * 100`%) with `r as.character(english(length(which(is.na(rrpp$data_avail)))))` articles not producing shareable data (Figure \@ref(fig:articleScores)D).

## Software Used

```{r softwareCited, echo=FALSE, include=FALSE}
# tally up software used for the next paragraph
rrpp_software <-
  rrpp %>% 
  transform(software_used = strsplit(software_used, ",")) %>%
  unnest(software_used) %>%
  mutate(software_used = trimws(software_used)) %>%
  mutate(software_used = toupper(software_used))

tab <- table(rrpp_software$software_used)
tab_s <- as.data.frame(sort(tab))
tab_s <-
  tab_s %>%
  arrange(desc(Freq)) %>%
  filter(Freq %in% head(unique(Freq), 10)) %>% 
  rename("Software" = "Var1", "Frequency" = "Freq")
```

There were `r as.character(english(length(unique(rrpp_software$software_used))))` unique software applications recorded being used in the articles that were evaluated.
These included desktop programs, web-based software and databases.
From the top ten most frequently cited software, the most frequently cited program was `r tab_s[1, 1]`, where different versions were not distinguished for this work.
The next three programs were statistical software, SAS (`r scales::ordinal(which(tab_s == "SAS"))`), SPSS (`r scales::ordinal(which(tab_s == "SPSS"))`) and R (`r scales::ordinal(which(tab_s == "R"))`) with two other statistical programs also frequently cited, GenStat and Statistica (tied 8th).
The remainder of the top ten programs we found were mostly related to sequence analysis, *i.e.*, Clustal (`r scales::ordinal(which(tab_s == "CLUSTAL"))`), BLAST (`r scales::ordinal(which(tab_s == "BLAST"))`), and BioEdit (9th) or phylogenetic analysis MrBayes (8th), tied with the most general-purpose software cited in the top 10, Excel®.
In tenth place was FigTree, used for phylogenetic trees.
The Python programming language was cited only twice.

## Statistical Analysis

When predicting the availability of computational methods (code) and availability, all journals were compared with *Phytopathology* as the reference with latent scores having a mean of zero.

There were no clear differences that the analysis could detect between any of the journals as compared with *Phytopathology* for the sharing of computational methods (code); the effects of all parameters were undecided (Figure \@ref(fig:journal-parameters)A, Figure S1, Table S1).

When predicting the availability of data availability as compared with *Phytopathology*, publications in two journals, *Phytopathologia Mediterranea* and *Molecular Plant Pathology*, can be considered clearly more likely to share data than publications in *Phytopathology*, but no effects were detectable in any other journal title (\@ref(fig:journal-parameters)B, Figure S2, Table S2).
The effect of *Molecular Plant Microbe Interactions* (Median = 0.67, 95% CI [-0.28, 1.57]) has a 92.15% probability of being positive (> 0), 90.61% of being significant (> 0.05), and 78.94% of being large (> 0.30).
The estimation successfully converged (Rhat = 1.000) and the indices are reliable (ESS = 25478).
The effect of *Phytopathologia Mediterranea* (Median = 1.25, 95% CI [0.32, 2.15]) has a 99.58% probability of being positive (> 0), 99.42% of being significant (> 0.05), and 97.73% of being large (> 0.30).
The estimation successfully converged (Rhat = 1.000) and the indices are reliable (ESS = 24986).

There was no effect detected for year when predicting the availability of computational methods (code) or data availability (Figures S3, S4; Tables S3, S4).

There were no detectable effects when predicting the effects of the five-year impact factor on the availability of computational methods (code) or data (Figures S5, S6, Tables S5, S6).

# Discussion

Except for a few isolated cases, most papers were not fully computationally reproducible.
Very few authors choose to share both data and code.
More authors shared data due to journal requirements to share sequence data, but other types of data related to field experiments or other laboratory studies were not likely to be shared.
Code sharing was extremely rare, but in cases where it was shared, most were included as a part of the journal article's publication as extra materials rather than through data repositories.
The reasons for not sharing code or data were not clearly available from the papers themselves and so this work was unable to determine possible reasons for this situation.
We recognize that in some cases there may be commercial or intellectual property (IP) reasons for not sharing data or code, but these reasons should be clearly stated.
However, in most cases, the data are collected with public funding and the code was developed using similar funding and there are no commercial or IP issues that preclude sharing and in fact the funding agency may have guidelines in place for sharing these materials.

Much of the software used are Free open-source Software (FOSS) packages, which means that the workflow can easily be recreated by anyone with the proper skills in using the software.
However, the top three most widely used software packages were not FOSS, limiting the ability of the authors to share workflows with others.
MEGA was the most widely cited software in this evaluation reflecting the widespread use of sequence analysis in the field of plant pathology.
The second, third and fourth, and eighth most frequently cited software packages were all statistical programs.
SAS remains firmly entrenched in the discipline as the choice of software for statistical analysis followed by SPSS but R, the most frequently occurring FOSS, is not far removed from SPSS in fourth place with Statistica and Genstat tied for eighth.
It is interesting to note however, that even with the popularity of Python in scientific programming it only appears in the survey twice in 2020 for both articles, not enough times to be in the top 10 most-cited.

## Examples of Different Levels of Reproducibility

There are different ways to make research more open and reproducible, which can be thought of as levels of reproducibility.
If a general workflow for producing academic research involves clearly defining a research question, obtaining data for testing the hypothesis, analyzing, summarizing and presenting data and results, writing the manuscript then the next logical steps are depositing the code used along with the raw data in proper repositories (Figure \@ref(fig:workflow-dia)).
Here we define three levels of reproducibility which are also related to the evolution of computational methods and reproducible practices and highlight papers that fit these definitions.

The first level is including tables of raw data or code with the paper as a supplemental file or even within the paper as tables where possible.
This is suitable for studies that may have a small data set or simple analysis or for demonstrations purposes as @Madden2015 demonstrate in their discussion regarding the use of $P$ values in statistical analyzes where they supply an *e-**X**tra* with reproducible examples for readers to refer to.
@Hill2019 also shared code and example data to reproduce the results of the paper as supplementary materials, citing all packages used and the versions.
The scripts provided used an R package, *checkpoint* [@Ooi2022], to provide a mechanism to help ensure reproducibility by installing the package versions used by the authors that are necessary to reproduce the work rather than defaulting to the latest versions.
However, the scripts did not run unhindered, as the supplementary materials suggested that they should, without changes to the data, but with some modifications to the data, and file name changes, the scripts ran allowing examples of the research to be reproduced using the definition of @Peng2009.
One drawback to this approach is that journals are often not equipped to handle code, e.g., script files, that may be developed as a part of the research process and so they are often archived as PDF or Word documents which hinder the ability to easily ingest and start working with them or if they are provided in a native text format, they do not render via the web properly.
Though this should be possible given some effort from the publishing journal to share simple text files rather than binary formats along with the proper instructions and handling of these file formats rather than a one-size-fits all approach that we commonly see for supplementary materials.
For example, the R scripts for @Hill2019 were provided as supplementary materials but do not actually appear in the browser window when requested, which may confuse readers and makes downloading them more difficult.
While this allows the reader to quickly view the extra materials and a DOI is assigned as a part of the article itself, the data or code are not readily findable and accessible. 
Furthermore, in many cases this does not allow prompt access to the data and running the code because of a journal paywall.

The second level is providing machine readable text files of the raw data and code in a public code repository such as GitHub, GitLab, but without a DOI or some other long-term repository as provided by a library, Zenodo, FigShare, etc.
Fewer authors choose to follow this method, but in one instance that we found, which was not a part of this analysis, @Vogel2021 deposited Fastq files in the National Center of Biotechnology Information Sequence Read Archive (BioProject accession number PRJNA616021) and provided the scripts for analysis via a GitHub repository (<https://github.com/gmv23/pcap-gwas>) but we were unable to find a DOI that refers to these materials or a proper citation to properly cite them here.

The third level is the use of proper code repositories such as GitHub and data repositories like FigShare, Zenodo or OSF.io among others, allowing for the deposition and updating of code, figures, data preprints or any other materials that support the article itself while providing a DOI and citation.
As an example, Sparks et al. [-@Sparks2011; -@Sparks2014] used FigShare to provide models, data and code [@Sparks2016] necessary to replicate model development and the subsequent study on the effects of climate change on potato late blight.
Similarly, @carleson2019population hosted the code for reproducing a population genomic analysis of *Phytophthora plurivora* on GitHub, while providing all data on OSF.io (the Open Science Framework).
@Lehner2017 used GitHub to host a code repository of their research compendium website with data and a reproducible report that explains in detail all steps of the analysis and the R code for conducting a meta-analysis for assessing heterogeneity in the relationship between white mold incidence and soybean yield and between incidence and soybean yield (<https://emdelponte.github.io/paper-white-mold-meta-analysis/>).
The website clearly demonstrates the analysis to readers and uses R so that anyone can easily replicate the work.
Using a public code repository resource like GitHub allows readers to easily contact the authors by opening "Issues" and report bugs or ask questions in an open forum that are not as straightforward when the data are provided as supplementary material.

Taking this approach even further, packaging the full analysis in a containerized software application, *e.g.*, Docker is a way to help ensure computational reproducibility but at the expense of added complexity.
Docker is an open-source containerization platform that enables users to package several applications and an operating system into containers, thereby standardizing the executable components by combining application source code (or analyzes) with the operating system required to run that analysis on a user's computer.
However, there are drawbacks to using Docker.
It can be difficult to understand for a new user and new platforms, like the Apple M1 chip, may not be fully supported, which hinder the ability to share the container.
But, in most cases using an open-source language like R, Julia or Python allows us to share our work in a fashion where we know that the analysis will run exactly the same on every computer.
For more on using Docker for reproducible research, we would refer readers to @Nuest2020.
As an example of this approach, @Khaliq2020 provided a research compendium as a Docker container with a DOI and a full R package that enables readers to fully replicate their analysis of *Ascochyta rabiei* conidia dispersal in chickpea using the data collected and stepping through other points where weather data were investigated and various models were fit before deciding on the best fit and recreate any figures as published in the article [@Khaliq2020a].
When this level is employed, tools such as Binder, <https://mybinder.org>, can be used that allow readers and reviewers to launch an interactive session in their web browser and interact with the data and rerun the analysis in an RStudio instance or Jupyter notebooks as Mioroni et al. used [-@Miorini2018; -@Miorini2019].
@Kamvar2015 took a slightly different approach by including all files necessary for the analysis and most output files in a repository [@Kamvar2014] that also included an installable R package that was used for the original analysis.
While there are many other methods, these two approaches illustrate some of the best-practices where the data and other files were deposited in repositories with DOIs and reproducibility issues were addressed by using R packages to handle dependencies and other versioning issues making the work more portable.

Efforts to annotate structured raw data (FAIR -- Findable, Accessible, Interoperable and Re-usable) [@Wilkinson2016] and fully document the analysis using open-source code which are deposited in public repositories and can be run by anyone following the download of data and code.
The first level, as reported, is an essential step that is not substituted by the other practices and eventually researchers fail to provide sufficient description or correct citations.
When making your science more open and reproducible, methods, software used (this includes items such as R, Julia or Python packages that were directly used in the analysis or production of the paper, etc.) should be cited properly.
Provide a full description of all equipment used, *e.g.*, "a Spectrum Technologies Watchdog 2700 weather station was used to record wind speed, direction, rainfall, temperature and relative humidity at one-hour intervals".
This allows end-users to identify what was used and identify the methods used more accurately.
Just as importantly, this acknowledges the contributions of others whose works were instrumental in your research.
This also helps ensure that you as a researcher can reconstruct what you have done since you will have good notes and documentation and be able to identify if something changes, *e.g.*, a package version, what effect it had on your research.

The use of programming or scripting languages such as R, Julia, SAS or Python enables you as a scientist to keep very detailed records of what was computationally performed.
This is as opposed to using software such as spreadsheet programs like Excel, Google Sheets, Numbers, Calc or others which can be used for simple statistical analyzes and visualization or other point-and-click software packages that do not enable you to keep an accurate record of the steps taken to import, format, visualize and analyze data.

Text files for saving small sets of data are preferable.
Data that are saved in binary formats such as PDF files are difficult to reuse because they are not easily machine readable.
In many cases, data sets are small enough and curated in spreadsheets, which should be saved as a plain text file, *e.g.*, comma separated (CSV) or tab separated (TSV) files.
This also helps ensure that the data are reusable.
Larger data sets may warrant the use of a proper database like lightweight personal databases, e.g., SQLite or DuckDB, larger more robust databases, e.g., MariaDB, PostgreSQL or a specialized database like GenBank which provide users with several benefits but two important benefits to mention here are (i) data redundancy, ensuring no records are duplicated and (ii) data consistency, ensuring that all records in a data set are recorded in the same format for every observation.
Databases such a GenBank are preferable for molecular data ensuring data integrity and machine readability.
While databases may offer many advantages for speed and data integrity, the trade-off is that they are more complex to set up and administer, especially for a small data set.

Ideally, once the data are complete, best practices for keeping your data as you perform your work include treating the raw data as read-only and using file permissions to prevent changes to the raw data files.
It should be noted that the use of a database management system also allows for both at the expense of added complexity.
Saving files in proprietary formats such as .xls(x) can also lead to issues in the future when opening using newer (or older) software versions.
Unexpected changes to values in the data [@Ziemann2016] may also occur when using proprietary formats.

If steps are followed to make the data FAIR, then it will be readable by humans and machines alike and this will help support discoveries and support further research.
In turn, sharing data will lead to new citations for your work as others discover and use it.
To make your data the most widely discoverable and usable, ensure that it has a persistent identifier.
A digital object identifier (DOI) is the most common (<https://www.doi.org/>) but the Handle.Net Registry (HNR) (<https://handle.net/>) is also an option.
There are different options for generating a DOI for your data and other materials.
FigShare, Zenodo and OSF.io all offer persistent archives along with a service to generate a DOI for your materials.
The use of a persistent identifier works to ensure that even if the data are moved, they can still be located using that unique identifier.
For more on FAIR data, visit Go-Fair <https://www.go-fair.org/fair-principles/>.

Once you have determined how to best manage your source code and the data sets for analysis the next step is to consider how to share your data.
Providers such as FigShare, Dataverse, OSF.io and Zenodo allow for you to deposit your data and generate a DOI for sharing your project once you are finished with it.

Other providers exist that allow for you to not only track changes but also to share the data openly, these include GitHub, Gitlab and Bitbucket.
GitHub is arguably the most popular and widely used software development platform currently.
Data that is encoded in CSV or other plain text formats can easily be deposited in a repository along with code for analysis to enable tracking of changes and other users to download and replicate the work.

We would advise against the practice of depositing data on a laboratory website or a site such as GitHub only as these are not an optimal way to preserve and share your work over the long term.
Doing either of these leaves the work in an unstable state where future users may be unable to access the work as they are fraught with link-rot and other issues.
It is a best practice to always ensure that you have deposited the data with a provider such as GenBank, Zenodo, FigShare or OSF.io and generated a DOI for the materials to help ensure continued accessibility.
Many of these providers provide rather easy ways to link the project with a software development repository to help ensure that the data are available in perpetuity.
If readers are uncertain, we suggest to also consult with local librarians about possible resources.
Most universities and other research-focused workplaces provide a facility for staff to deposit papers and other academic materials, but this may extend to software development or data repositories as well.
The important aspect is that data once deposited cannot be modified any more.

## Final Thoughts

In this work we have evaluated the state of computational reproducibility in the plant pathology literature and presented suggestions for areas of improvement.
As we prepared this letter, we became more aware of the urgent need to spread and establish an open science attitude and culture among plant pathologists.
To assist in fostering this sort of change in our discipline, Open Plant Pathology (OPP), <https://www.openplantpathology.org>, an institution-independent and non-funded initiative, was founded in January 2018 by two of this letter's co-authors, Del Ponte and Sparks, in 2018 with the following vision: "foster a diverse community culture that values openness, transparency and reproducibility of scientific research data and methods in our field".
We started OPP with a minimal infrastructure and support from other enthusiastic colleagues that allows members to interact sharing and gathering ideas on how we can improve the openness and reproducibility in our discipline.

We believe that adopting an attitude of open and collaborative science and using the best reproducibility practices in our daily work, directly benefits us as researchers.
For example, between complicated analyzes, reviews and revisions and questions years later about the data that was collected or analysis that was conducted, it is extremely beneficial to be able to easily reproduce your work quickly and easily.
This manuscript was drafted over the course of several years as the authors had time to devote to it.
Having everything in a reproducible framework made it easy to resume work and set-aside as necessary without losing information and having everything well-documented made it simpler to do this.
Second, it benefits the reviewer by aiding their understanding of the work done and gives them more materials to use to make suggestions for improvements when reviewing and the end-user or reader is better able to verify the validity of the methods used and recreate the analysis.
Perhaps more importantly, sharing these details helps with knowledge transfer by showing other interested parties how something was done rather than simply describing it.
Lastly, openly sharing your work and making it discoverable can lead to new collaborations and synergistic ideas.
One of the most important messages that we would like to share is that there is more to the work than just the paper.
Sharing materials detailing the analysis that was performed and documenting the data provide citable products and enhance the manuscript providing the reader with a richer set of information with which to understand the work that was performed.
This open sharing of code and data leads to greater impact as work is cited if resources such as code or data are reused.
But it is not just up to the authors to ensure that their work is reproducible.
At the very least, journals can and should provide clear instruction for how to deposit the data and code in a repository and mint a DOI to accompany these resources to encourage authors to share the data and code that support the manuscript.
Ideally, with mandates for openly sharing data becoming more common with funding agencies, journals should also be mandating this practice as well.
We can and should embrace this and move the discipline forward and have a greater impact with our work.

Without the code used to generate the paper and the data that were gathered for the paper, we as a community cannot adequately evaluate the work as it is presented.
That is, while we may be standing on the shoulders of giants in doing our work, we cannot see where they have been to do our work, which hinders our efforts to be able to see farther ahead.

# Authors' Contributions

A.H.S, E.DP., Z.F. and N.J.G. conceived of the presented idea and scoring system.
E.DP. provided five-year impact factors.
A.H.S., E.DP., Z.F. K.A. and N.G. evaluated articles for scoring.
A.H.S. designed the computational framework for analysis and created the research compendium with feedback from E.DP. and Z.F.
A.H.S. wrote the manuscript in consultation with E.DP.
All authors provided critical feedback and helped contribute to the final version of the manuscript.

# Data and Code Availability

## Data

The raw data for this work are documented and available from DOI [https://doi.org/10.5281/zenodo.4941722](10.5281/zenodo.4941722).

## Code

All code used in the analyzes and data visualization and associated materials have been made available as research compendium available from DOI [https://doi.org/10.5281/zenodo.1250664](10.5281/zenodo.1250664).
A webpage version of the compendium is available from <https://openplantpathology.github.io/Reproducibility_in_Plant_Pathology/>.

# Acknowledgements

The authors are grateful for insightful comments from Drs. David Ferris and Rebecca O'Leary the USQ Centre for Crop Health Advisory Group on the final manuscript that helped improve the paper and from Ms. Anna Hepworth for statistical guidance on evaluating inter-rater scores.

\newpage

# Literature Cited

<!-- The following line ensures the references appear here for the MS Word or HTML output files, rather than right at the end of the document (this will not work for PDF files):  -->

::: {#refs}
:::

\newpage

# Tables

```{r journals, echo=FALSE, message=FALSE, warning=FALSE}
journal <-
  tabyl(rrpp, journal) %>%
  select(-percent)

distinct(rrpp, journal, IF_5year) %>%
  left_join(journal, by = "journal") %>% 
  rename(Journal = journal,
         `5 Year IF` = IF_5year) %>%
  flextable() %>%
  colformat_num(na_str = "NA") %>%
  autofit() %>%
  set_caption(
    "Journal titles selected for inclusion, their respective five-year impact factors as of 2022 (5 Year IF) and the number (n) of articles from each journal that were evaluated."
  )
```

\newpage

```{r software-used, echo=FALSE, message=FALSE, warning=FALSE}
tab_t <- tabyl(rrpp_software, software_used, year)

tab_t <- tab_t[tab_t$software_used %in% tab_s$Software ,]

tab_t %>%
  rowwise() %>%
  mutate(Total = sum(c_across(where(is.numeric)), na.rm = T)) %>%
  arrange(desc(Total)) %>%
  rename(Software = software_used) %>% 
  flextable() %>%
  autofit() %>%
  set_caption(
    "The frequency of use of top ten software programs by year of publication that were found to be used in the papers that were surveyed."
  )
```

\newpage

# Figures

```{r posteriors, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Posterior distribution visualizations for each of six models fitted to scoring data that were used to evaluate factors on reproducibility of 450 papers published in 21 plant pathology journals or plant pathology focused articles from other specialized journals. The journal title was tested for an effect on code availability (A) and data availability (B), the year of publication was tested for an effect on code availability (C) and data availability (D), and five-year impact factor of journal was tested for effect on code availability (E) and data availability (F). Model fitnesses were found to be good for all models.", cache=FALSE, fig.env='figure'}

set.seed(23)

color_scheme_set(scheme = "gray")

mod_list <- list(
  pp_check(m_f1, type = "rootogram", ndraws = 50) +
    theme(
      axis.text.x = element_blank(),
      axis.ticks.x = element_blank(),
      axis.title.x = element_blank()
    ),
  pp_check(m_f2, type = "rootogram", ndraws = 50) +
    theme(
      axis.text.x = element_blank(),
      axis.ticks.x = element_blank(),
      axis.title.x = element_blank(),
      axis.text.y = element_blank(),
      axis.ticks.y = element_blank(),
      axis.title.y = element_blank()
    ),
  pp_check(m_g1, type = "rootogram", ndraws = 50) +
    theme(
      axis.text.x = element_blank(),
      axis.ticks.x = element_blank(),
      axis.title.x = element_blank()
    ),
  pp_check(m_g2, type = "rootogram", ndraws = 50) +
    theme(
      axis.text.x = element_blank(),
      axis.ticks.x = element_blank(),
      axis.title.x = element_blank(),
      axis.text.y = element_blank(),
      axis.ticks.y = element_blank(),
      axis.title.y = element_blank()
    ),
  pp_check(m_h1, type = "rootogram", ndraws = 50),
  pp_check(m_h2, type = "rootogram", ndraws = 50) +
    theme(
      axis.text.y = element_blank(),
      axis.ticks.y = element_blank(),
      axis.title.y = element_blank()
    )
)

p <- wrap_plots(mod_list, ncol = 2) +
  plot_layout(guides = "collect") &
  plot_annotation(title = "Bayesian posterior distributions of evaluation score models",
                  tag_levels = "A") &
  theme(text = element_text("Arial"))

ggsave(
  filename = here("analysis/figures/Sparks_et_al_Figure_1.eps"),
  plot = p,
  width = 6,
  height = 6,
  units = "in",
  device = cairo_ps
)
embed_fonts(
  file = here("analysis/figures/Sparks_et_al_Figure_1.eps"),
  outfile = here("analysis/figures/Sparks_et_al_Figure_1.eps"),
  options = "-dEPSCrop"
)
include_graphics(path = here("analysis/figures/Sparks_et_al_Figure_1.eps"))
```

\newpage

```{r articleScores, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Aggregated article scores for each of the two categories evaluated, (A) displays 'Code availability', where '0' was 'Not available or not mentioned in the publication'; '1' was 'Available upon request to the author; '2' was 'Online, but inconvenient or non-permanent (e.g., login needed, paywall, FTP server, personal lab website that may disappear, or may have already disappeared)'; and '3' was 'Freely available online to anonymous users for foreseeable future (e.g., archived using Zenodo, dataverse or university library or some other proper archiving system)'; 'NA' indicates that no code was created to conduct the work that was detectable. (B) shows 'Data availability', where '0' was 'Not available or not mentioned in the publication'; '1' was 'Available upon request to the author; '2' was 'Online, but inconvenient or non-permanent (e.g., login needed, paywall, FTP server, personal lab website that may disappear, or may have already disappeared)'; and '3' was 'Freely available online to anonymous users for foreseeable future (e.g., archived using Zenodo, dataverse or university library or some other proper archiving system)'; 'NA' indicates that no data were generated, e.g., a methods paper.", cache=FALSE, fig.env='figure'}

a <- ggplot(rrpp, aes(x = as.factor(comp_mthds_avail))) +
  geom_bar(fill = "black") +
  ylab("Count") +
  xlab("Article Score") +
  ggtitle("Code")

b <- ggplot(rrpp, aes(x = as.factor(data_avail))) +
  geom_bar(fill = "black") +
  ylab("Count") +
  xlab("Article Score") +
  ggtitle("Data")

p <- a + b
p <- p +
  plot_annotation(tag_levels = "A") &
  theme(text = element_text("Arial"))

ggsave(
  filename = here("analysis/figures/Sparks_et_al_Figure_2.eps"),
  plot = p,
  width = 6,
  height = 6,
  units = "in",
  device = cairo_ps
)
embed_fonts(
  file = here("analysis/figures/Sparks_et_al_Figure_2.eps"),
  outfile = here("analysis/figures/Sparks_et_al_Figure_2.eps"),
  options = "-dEPSCrop"
)
include_graphics(path = here("analysis/figures/Sparks_et_al_Figure_2.eps"))
```

\newpage

```{r journal-parameters, echo=FALSE, message=FALSE, warning=FALSE, fig.cap='95% credible interval for the probability associated with the journal title for (A) computational materials (code) and (B) data being made readily available to the public. A test for equivalence was unable to detect any clear differences between *Phytopathology* and all other journals sharing computational materials (code). However, a test for equivalence found that *Molecular Plant Pathology* and *Phytopathologia Mediterranea* were more likely have data available than articles found in *Phytopathology*. Intervals in grey have a median value less than that of *Phytopathology*, the base level used for the analysis and articles in these journals could be less likely to share computational materials and data than articles published in *Phytopathology*. Intervals in black have a median value that is greater than that of *Phytopathology*, the base level for the analysis and articles in these journals could be more likely to share data or code than those found in *Phytopathology*.', cache=FALSE, fig.env='figure'}

a_y_labs <- rev(gsub("b_abbreviation", "", parameters(m_f1)$Parameter)[-c(1:2)])
a <- plot(parameters(m_f1)) +
  scale_y_discrete(labels = a_y_labs) +
  scale_colour_grey() +
  ggtitle("Code")

b <- plot(parameters(m_f2)) +
  scale_colour_grey() +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.title.y = element_blank()
  ) +
  ggtitle("Data")

p <- a + b
p <- p +
  plot_annotation(tag_levels = "A") &
  theme(text = element_text("Arial"))

ggsave(
  filename = here("analysis/figures/Sparks_et_al_Figure_3.eps"),
  plot = p,
  width = 6,
  height = 6,
  units = "in",
  device = cairo_ps
)
embed_fonts(
  file = here("analysis/figures/Sparks_et_al_Figure_3.eps"),
  outfile = here("analysis/figures/Sparks_et_al_Figure_3.eps"),
  options = "-dEPSCrop"
)
include_graphics(path = here("analysis/figures/Sparks_et_al_Figure_3.eps"))
```

\newpage

```{r year-parameters, echo=FALSE, message=FALSE, warning=FALSE, fig.cap='95% credible interval for the probability associated with the year of publication for (A) computational materials (code) and (B) data being made readily available to the public. A test for practical equivalence was undecided in any detectable effects of year of publication on computational materials or data being shared. Intervals in grey indicate that the median was less than zero or less likely that code or data were shared and intervals in black indicate that the median value is above zero or more likely to share data and code.', cache=FALSE, fig.env='figure'}
a <- plot(parameters(m_g1), show_lables = TRUE) + 
  scale_colour_grey() +
  ggtitle("Code")

b <- plot(parameters(m_g2), show_lables = TRUE) + 
  scale_colour_grey() +
   theme(
      axis.text.y = element_blank(),
      axis.ticks.y = element_blank(),
      axis.title.y = element_blank()
    ) +
  ggtitle("Data")

p <- a + b
p <- p +
  plot_annotation(tag_levels = "A") &
  theme(text = element_text("Arial"))

ggsave(
  filename = here("analysis/figures/Sparks_et_al_Figure_4.eps"),
  plot = p,
  width = 6,
  height = 6,
  units = "in",
  device = cairo_ps
)
embed_fonts(
  file = here("analysis/figures/Sparks_et_al_Figure_4.eps"),
  outfile = here("analysis/figures/Sparks_et_al_Figure_4.eps"),
  options = "-dEPSCrop"
)
include_graphics(path = here("analysis/figures/Sparks_et_al_Figure_4.eps"))
```

\newpage

```{r IF-parameters, echo=FALSE, message=FALSE, warning=FALSE, fig.cap='95% credible interval for the probability associated with the publishing journal\'s five-year impact factor for (A) computational materials (code) and (B) data being made readily available to the public. A test for practical equivalence was undecided on any detectable effects of impact factor. Intervals in grey indicate that the median was less than zero or less likely that code or data were shared and intervals in black indicate that the median value is above zero or more likely to share data and code.', cache=FALSE, fig.env='figure'}
a <- plot(parameters(m_h1), show_lables = TRUE) + 
  scale_colour_grey() +
  ggtitle("Code")

b <- plot(parameters(m_h2), show_lables = TRUE) + 
  scale_colour_grey() +
   theme(
      axis.text.y = element_blank(),
      axis.ticks.y = element_blank(),
      axis.title.y = element_blank()
    ) +
  ggtitle("Data")

p <- a + b
p <- p +
  plot_annotation(tag_levels = "A") &
  theme(text = element_text("Arial"))

ggsave(
  filename = here("analysis/figures/Sparks_et_al_Figure_5.eps"),
  plot = p,
  width = 6,
  height = 6,
  units = "in",
  device = cairo_ps
)
embed_fonts(
  file = here("analysis/figures/Sparks_et_al_Figure_5.eps"),
  outfile = here("analysis/figures/Sparks_et_al_Figure_5.eps"),
  options = "-dEPSCrop"
)
include_graphics(path = here("analysis/figures/Sparks_et_al_Figure_5.eps"))
```

\newpage

```{r workflow-dia, echo=FALSE, message=FALSE, warning=FALSE, fig.cap='An example of an open and reproducible research workflow. Starting with the question, determine the methodology, describe it, make it available and cite it. Data are used in analysis and any binary files or code are made available as supplements to the manuscript. Source code and raw data are made available in a public repository, preferably with version control for tracking changes through time, and a DOI for final released products.', cache=FALSE, fig.env='figure'}

p <- export_svg(
  grViz(
    "digraph Fig3 {
    graph [fontsize = 8, nodesep=0.5]

    node[shape = plaintext, width = 1.2, fontname = Helvetica, fontsize = 20]
    'An open and reproducible workflow'

    node [shape = oval, style = filled, fillcolor = grey99, width = 1.2, fontname = Arial]
    Question

    node [shape = box, style = filled, fillcolor = grey99, width = 1.2, fontname = Arial]
    Methodology; Data; Analysis; Manuscript;

    node [shape = box, style = filled, fillcolor = grey90, width = 1.2, fontname = Arial]
    Description; Availability; Citation

    node [shape = box, style = filled, fillcolor = grey80, width = 1.2]
    'Binary Code'; 'Binary File'; Supplement

    node [shape = box, style = filled, fillcolor = grey70, width = 1.2]
    'Source Code'; 'Data File'; Metadata; 'Public Repository';

    Question->Methodology->Data->Analysis
    Analysis->Manuscript
    Methodology->Description
    Methodology->Citation
    Methodology->Availability
    Manuscript->Supplement
    Data->'Data File'->'Public Repository'
    Data->Metadata->'Public Repository'
    Analysis->'Binary Code'->Supplement
    Analysis->'Source Code'->'Public Repository'
    Data->'Binary File'->Supplement
  }",
  width = 1000,
  height = 750
  )
)

rsvg_ps(svg = charToRaw(p),
        file = here("analysis/figures/Sparks_et_al_Figure_6.eps"))
embed_fonts(
  file = here("analysis/figures/Sparks_et_al_Figure_6.eps"),
  outfile = here("analysis/figures/Sparks_et_al_Figure_6.eps"),
  options = "-dEPSCrop"
)

include_graphics(path = here("analysis/figures/Sparks_et_al_Figure_6.eps"))
```
