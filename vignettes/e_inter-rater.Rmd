---
title: "Inter-rater Repeatability"
author: "Adam H. Sparks"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Inter-rater Repeatability}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\VignetteDepends{tidyverse}
  %\VignetteDepends{irr}
  %\VignetteDepends{pander}
  %\VignetteDepends{ggridges}
  %\VignetteDepends{patchwork}
  %\VignetteDepends{psych}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
knitr::opts_chunk$set(fig.width = 6.4, fig.height = 6.4)
```

This vignette documents the analysis of the data gathered from surveying 5 journal articles, which all five authors evaluated for an inter-rater comparison.
Five articles were evaluated by all five authors to understand the differences between ratings assigned by each of the paper's authors.
All original scores are kept with corrections being made to the `software_avail` column in `software_avail_corrected` (by AHS) based on the software that was recorded as being used in each paper.
This step removes some disagreement between the ratings.

# Set-up Workspace

Load libraries used and setting the *ggplot2* theme for the document.

```{r load-libraries, message=FALSE}
library(ggridges)
library(irr)
library(patchwork)
library(psych)
library(tidyverse)
library(pander)
library(Reproducibility.in.Plant.Pathology)

theme_set(theme_classic())
```

## Import Data

Import the data and print.

```{r import-rrpp}
rrpp <- import_interrater_scores()
pander(rrpp)
```

## Article Classifications

Check differences between evaluators in classifying articles as having a molecular focus or fundamental or applied research.

```{r article-classifications}
a <- ggplot(rrpp, aes(y = doi,
                      fill = molecular)) +
  geom_bar() +
  scale_fill_grey(name = "") +
  ylab("DOI") +
  xlab("Count") +
  ggtitle("Molecular Focus") +
  theme(legend.position = "top")

b <- ggplot(rrpp, aes(y = doi,
                      fill = art_class)) +
  geom_bar() +
  scale_fill_grey(name = "") +
  xlab("Count") +
  ylab("") +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  ggtitle("Article Class") +
  theme(legend.position = "top")

p <- (a | b)
p
```

## Reproducibility Criteria

Visualise differences between evaluators for the four criteria used to score reproducibility for each paper.

```{r reproducibility-scores}
c <- ggplot(rrpp,
            aes(y = doi,
                x = software_cite)) +
  ylab("DOI") +
  xlab("") +
  xlim(c(0, 3)) +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) +
  geom_density_ridges() +
  ggtitle("Software Cited")

d <- ggplot(rrpp,
            aes(y = doi,
                x = software_avail)) +
  ylab("") +
  xlab("") +
  geom_density_ridges() +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  xlim(c(0, 3)) +
  ggtitle("Software Available")

e <- ggplot(rrpp,
            aes(y = doi,
                x = comp_mthds_avail)) +
  geom_density_ridges() +
  ylab("DOI") +
  xlab("Score") +
  xlim(c(0, 3)) +
  ggtitle("Comp. Methods Available")

f <- ggplot(rrpp,
            aes(y = doi,
                x = data_avail)) +
  ylab("") +
  xlab("Score") +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  xlim(c(0, 3)) +
  geom_density_ridges() +
  ggtitle("Data Available")

p <- (c | d) / (e | f)
p
```

## Reproducibility Scores

Look at agreement of the reproducibility scores across evaluators with corrected `software_avail` ratings, `software_avail_corrected`.

```{r repro-scores}
ggplot(rrpp,
       aes(y = doi,
           x = reproducibility_score)) +
  geom_density_ridges() +
  ylab("DOI") +
  xlab("Reproducibility Score (%)") +
  xlim(c(0, 100))
```

## Software Used

We can check if most authors found the same software in the papers that were evaluated.
Perfect agreement is a 5, below that the software was not clearly and easily detected by the evaluator.

```{r software-table}
rrpp_software <-
  rrpp %>%
  transform(software_used = strsplit(software_used_cleaned, ",")) %>%
  unnest(software_used) %>%
  mutate(software_used = trimws(software_used)) %>%
  mutate(software_used = toupper(software_used))

tab <- table(rrpp_software$software_used)
tab_s <- as.data.frame(sort(tab))
tab_s <-
  tab_s %>%
  arrange(desc(Freq)) %>%
  rename("Software" = "Var1", "Frequency" = "Freq")

pander(tab_s)
```

## Agreement Stats

Here will compute a simple percentage agreement between the reviewers using the [*irr*](https://CRAN.R-project.org/package=irr) package.

### Software Citations

Percent agreement of software citations criteria rankings.

##### Agreement

```{r software-cite-agreement, warning=FALSE}
rrpp %>% 
  select(doi, assignee, software_cite) %>% 
  pivot_wider(names_from = assignee, values_from = software_cite) %>% 
  select(Adam, Emerson, Kaique, Nik, Zach) %>% 
  as.matrix() %>% 
  agree() %>% 
  pander()
```

#### Fleiss' Kappa

```{r software-cite-kappa, warning=FALSE}
rrpp %>% 
  select(doi, assignee, software_cite) %>% 
  pivot_wider(names_from = assignee, values_from = software_cite) %>% 
  select(Adam, Emerson, Kaique, Nik, Zach) %>% 
  as.matrix() %>% 
  kappam.fleiss() %>% 
  pander()
```

### Data Availability

##### Agreement

Percent agreement of data availability criteria rankings.

```{r data-avail-agreement, warning=FALSE}
rrpp %>% 
  select(doi, assignee, data_avail) %>% 
  pivot_wider(names_from = assignee, values_from = data_avail) %>% 
  select(Adam, Emerson, Kaique, Nik, Zach) %>% 
  as.matrix() %>% 
  agree() %>% 
  pander()
```

### Computational Availability

##### Agreement

Percent agreement of computational methods availability criteria rankings.

```{r comp-methods-agreement, warning=FALSE}
rrpp %>% 
  select(doi, assignee, comp_mthds_avail) %>% 
  pivot_wider(names_from = assignee, values_from = comp_mthds_avail) %>% 
  select(Adam, Emerson, Kaique, Nik, Zach) %>% 
  as.matrix() %>% 
  agree() %>% 
  pander()
```

### Reproduciblity Score

Calculate the intraclass correlation (ICC), a measure of association to rate the reliability of raters.
See `?psych::ICC` for more on this function and the methods.

```{r repro-score-icc}
rrpp %>% 
  select(doi, assignee, reproducibility_score) %>% 
  pivot_wider(names_from = assignee, values_from = reproducibility_score) %>% 
  select(Adam, Emerson, Kaique, Nik, Zach) %>% 
  as.matrix() %>% 
  ICC()
```

[Koo & Li](https://pubmed.ncbi.nlm.nih.gov/27330520/) provide the following guidelines for interpreting the ICC:

-   **Less than 0.50**: Poor reliability
-   **Between 0.5 and 0.75**: Moderate reliability
-   **Between 0.75 and 0.9**: Good reliability
-   **Greater than 0.9**: Excellent reliability

Thus, we would conclude that an ICC of **0.57** indicates that the papers can be rated with "moderate" reliability by different raters.

# Colophon

```{r sessioninfo}
sessioninfo::session_info()
```
