---
title: "Inter-rater Repeatability"
author: "Adam H. Sparks"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Inter-rater Repeatability}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\VignetteDepends{tidyverse}
  %\VignetteDepends{irr}
  %\VignetteDepends{pander}
  %\VignetteDepends{ggridges}
  %\VignetteDepends{patchwork}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
knitr::opts_chunk$set(fig.width = 6.4, fig.height = 6.4)
```

This vignette documents the analysis of the data gathered from surveying 5 journal articles, which all five authors evaluated for an inter-rater comparison.
Five articles were evaluated by all five authors to understand the differences between ratings assigned by each of the paper's authors.
All original scores are kept with corrections being made to the `software_avail` column in `software_avail_corrected` (by AHS) based on the software that was recorded as being used in each paper.
This step removes some disagreement between the ratings.

# Set-up Workspace

Load libraries used and setting the *ggplot2* theme for the document.

```{r load-libraries, message=FALSE}
library(ggridges)
library(irr)
library(patchwork)
library(psych)
library(tidyverse)
library(pander)
library(Reproducibility.in.Plant.Pathology)

theme_set(theme_classic())
```

## Import Data

Import the data.

```{r import-rrpp}
rrpp <- import_interrater_scores()
```

## Article Classifications

Check differences between evaluators in classifying articles as having a molecular focus or fundamental or applied research.

```{r article-classifications}
a <- ggplot(rrpp, aes(y = doi,
                      fill = molecular)) +
  geom_bar() +
  scale_fill_grey(name = "") +
  ylab("DOI") +
  xlab("Count") +
  ggtitle("Molecular Focus") +
  theme(legend.position = "top")

b <- ggplot(rrpp, aes(y = doi,
                      fill = art_class)) +
  geom_bar() +
  scale_fill_grey(name = "") +
  xlab("Count") +
  ylab("") +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  ggtitle("Article Class") +
  theme(legend.position = "top")

p <- (a | b)
p
```

## Reproducibility Criteria

Visualise differences between evaluators for the four criteria used to score reproducibility for each paper.

```{r reproducibility-scores}
c <- ggplot(rrpp,
            aes(y = doi,
                x = software_cite)) +
  ylab("DOI") +
  xlab("") +
  xlim(c(0, 3)) +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) +
  geom_density_ridges() +
  ggtitle("Software Cited")

d <- ggplot(rrpp,
            aes(y = doi,
                x = software_avail)) +
  ylab("") +
  xlab("") +
  geom_density_ridges() +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  xlim(c(0, 3)) +
  ggtitle("Software Available")

e <- ggplot(rrpp,
            aes(y = doi,
                x = comp_mthds_avail)) +
  geom_density_ridges() +
  ylab("DOI") +
  xlab("Score") +
  xlim(c(0, 3)) +
  ggtitle("Comp. Methods Available")

f <- ggplot(rrpp,
            aes(y = doi,
                x = data_avail)) +
  ylab("") +
  xlab("Score") +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  xlim(c(0, 3)) +
  geom_density_ridges() +
  ggtitle("Data Available")

p <- (c | d) / (e | f)
p
```

## Software Used

We can check if most authors found the same software in the papers that were evaluated.
Perfect agreement is a 5, below that the software was not clearly and easily detected by the evaluator.

```{r software-table}
rrpp_software <-
  rrpp %>%
  transform(software_used = strsplit(software_used_cleaned, ",")) %>%
  unnest(software_used) %>%
  mutate(software_used = trimws(software_used)) %>%
  mutate(software_used = toupper(software_used))

tab <- table(rrpp_software$software_used)
tab_s <- as.data.frame(sort(tab))
tab_s <-
  tab_s %>%
  arrange(desc(Freq)) %>%
  rename("Software" = "Var1", "Frequency" = "Freq")

pander(tab_s)
```

## Agreement Stats

Here will compute a simple percentage agreement between the reviewers using the [*irr*](https://CRAN.R-project.org/package=irr) package.

### Software Citations

Percent agreement of software citations criteria rankings.

##### Agreement

```{r software-cite-agreement, warning=FALSE}
rrpp %>% 
  select(doi, assignee, software_cite) %>% 
  pivot_wider(names_from = assignee, values_from = software_cite) %>% 
  select(Adam, Emerson, Kaique, Nik, Zach) %>% 
  as.matrix() %>% 
  agree() %>% 
  pander()
```

#### Fleiss' Kappa

```{r software-cite-kappa, warning=FALSE}
rrpp %>% 
  select(doi, assignee, software_cite) %>% 
  pivot_wider(names_from = assignee, values_from = software_cite) %>% 
  select(Adam, Emerson, Kaique, Nik, Zach) %>% 
  as.matrix() %>% 
  kappam.fleiss() %>% 
  pander()
```

### Data Availability

##### Agreement

Percent agreement of data availability criteria rankings.

```{r data-avail-agreement, warning=FALSE}
rrpp %>% 
  select(doi, assignee, data_avail) %>% 
  pivot_wider(names_from = assignee, values_from = data_avail) %>% 
  select(Adam, Emerson, Kaique, Nik, Zach) %>% 
  as.matrix() %>% 
  agree() %>% 
  pander()
```

### Computational Availability

##### Agreement

Percent agreement of computational methods availability criteria rankings.

```{r comp-methods-agreement, warning=FALSE}
rrpp %>% 
  select(doi, assignee, comp_mthds_avail) %>% 
  pivot_wider(names_from = assignee, values_from = comp_mthds_avail) %>% 
  select(Adam, Emerson, Kaique, Nik, Zach) %>% 
  as.matrix() %>% 
  agree() %>% 
  pander()
```

# Colophon

```{r sessioninfo}
sessioninfo::session_info()
```
